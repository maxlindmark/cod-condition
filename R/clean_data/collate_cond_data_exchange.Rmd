---
title: "Collate condition data"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  df_print: paged
pdf_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.asp = 0.618,
  fig.align ='center'
)
```

# Intro
In this script, I load exchange data from datras and extract CA data (i.e. biological data), which is used for the condition model. In theory I could have joined the CPUE data to the CA data and get all important covariates and variables (oxygen, depth, ices information, densities of cod and flounder). However, the CPUE data has been standardized with respect to trawl speed, gear dimension, sweep length and trawl duration. Because many haul id's did not have this information, the CPUE data has fewer id's. In order to not lose condition data because I don't have haul-level CPUE of cod and flounder, I will instead repeat the data cleaning process here, fit models to cod and flounder CPUE and predict at the location of the condition data.

## Load libraries

```{r, message=FALSE}
rm(list = ls())

library(tidyverse); theme_set(theme_light(base_size = 12))
library(readxl)
library(tidylog)
library(RCurl)
library(viridis)
library(RColorBrewer)
library(patchwork)
library(janitor)
library(icesDatras)
library(mapdata)
library(patchwork)
library(rgdal)
library(raster)
library(sf)
library(rgeos)
library(chron)
library(lattice)
library(ncdf4)
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(mapplots)
library(geosphere)
#remotes::install_github("pbs-assess/sdmTMB")
library(sdmTMB)
```

## For maps

```{r read coastline data, message=FALSE, warning=FALSE}
world <- ne_countries(scale = "medium", returnclass = "sf")

# Specify map ranges
ymin = 54; ymax = 58; xmin = 12; xmax = 22

map_data <- rnaturalearth::ne_countries(
  scale = "medium",
  returnclass = "sf", continent = "europe")

# Crop the polygon for plotting and efficiency:
# st_bbox(map_data) # find the rough coordinates
swe_coast <- suppressWarnings(suppressMessages(
  st_crop(map_data,
          c(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax))))

# Transform our map into UTM 33 coordinates, which is the equal-area projection we fit in:
utm_zone33 <- 32633
swe_coast_proj <- sf::st_transform(swe_coast, crs = utm_zone33)

#ggplot(swe_coast_proj) + geom_sf() 
```

## Read data

```{r, message=FALSE}
# Read HH data
bits_hh <- read.csv("data/DATRAS_exchange/bits_hh.csv") %>% filter(Quarter == 4)

# Read CA data
bits_ca <- read.csv("data/DATRAS_exchange/bits_ca.csv") %>% filter(Quarter == 4)
```

## Standardize catch data
### Standardize ships

```{r, message=FALSE}
# Before creating a a new ID, make sure that countries and ships names use the same format
sort(unique(bits_hh$Ship))

# Change back to the old Ship name standard...
# https://vocab.ices.dk/?ref=315
# https://vocab.ices.dk/?ref=315
# Assumptions:
# SOL is Solea on ICES links above, and SOL1 is the older one of the two SOLs (1 and 2)
# DAN is Dana
# sweep %>% filter(Ship == "DANS") %>% distinct(Year, Country)
# sweep %>% filter(Ship == "DAN2") %>% distinct(Year)
# bits_hh %>% filter(Ship == "67BC") %>% distinct(Year, Country)
# sweep %>% filter(Ship == "DAN2") %>% distinct(Year)
# bits_hh %>% filter(Ship == "26D4") %>% distinct(Year) # Strange that 26DF doesn't extend far back. Which ship did the Danes use? Ok, I have no Danish data that old.
# bits_hh %>% filter(Country == "DK") %>% distinct(Year)

bits_hh <- bits_hh %>%
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) %>% 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_ca <- bits_ca %>%
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) %>% 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))
```

### Standardize countries

```{r, message=FALSE}
# Now check which country codes are used
sort(unique(bits_hh$Country))

# https://www.nationsonline.org/oneworld/country_code_list.htm#E
bits_hh <- bits_hh %>%
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_ca <- bits_ca %>%
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

# Gear? Are they the same?
sort(unique(bits_hh$Gear))
```

### Create a simple ID that works across all exchange data

```{r, message=FALSE}
# Create ID column
bits_ca <- bits_ca %>% 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hh <- bits_hh %>% 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Works like a haul-id
# bits_hh %>% group_by(IDx) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
```

### Create the same unique haul-ID in the cpue data that I have in the sweep-file

```{r, message=FALSE}
bits_hh <- bits_hh %>% 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh %>% 
  distinct(haul.id, .keep_all = TRUE) %>% 
  group_by(Month) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate((prop = n / sum(n)) * 100)
              
  

```

### Clean DATRAS EXCHANGE data

```{r, message=FALSE}
# Select just valid, additional and no oxygen hauls
bits_hh <- bits_hh %>%
  filter(HaulVal %in% c("A","N","V"))

# Add ICES rectangle
bits_hh$Rect <- mapplots::ices.rect2(lon = bits_hh$ShootLong, lat = bits_hh$ShootLat)

# Add ICES subdivisions
shape <- shapefile("data/ICES_StatRec_mapto_ICES_Areas/StatRec_map_Areas_Full_20170124.shp")

pts <- SpatialPoints(cbind(bits_hh$ShootLong, bits_hh$ShootLat), 
                     proj4string = CRS(proj4string(shape)))

bits_hh$sub_div <- over(pts, shape)$Area_27

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(bits_hh$sub_div))

bits_hh <- bits_hh %>% 
  mutate(sub_div = factor(sub_div),
         sub_div = fct_recode(sub_div,
                              "20" = "3.a.20",
                              "21" = "3.a.21",
                              "22" = "3.c.22",
                              "23" = "3.b.23",
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2",
                              "29" = "3.d.29",
                              "30" = "3.d.30"),
         sub_div = as.character(sub_div)) 

# Match columns from the HH data to the HL and CA data
sort(unique(bits_hh$sub_div))
sort(colnames(bits_hh))
bits_hh_merge <- bits_hh %>% 
                       dplyr::select(sub_div, Rect, HaulVal, StdSpecRecCode, BySpecRecCode,
                                     IDx, ShootLat, ShootLong, Month, Day)

bits_ca <- left_join(bits_ca, bits_hh_merge, by = "IDx")

# Now filter the subdivisions I want from all data sets
bits_hh <- bits_hh %>% filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_ca <- bits_ca %>% filter(sub_div %in% c(24, 25, 26, 27, 28))
```

```{r test}
library(lubridate)

bits_ca$Date <- as.Date(paste(bits_ca$Year, bits_ca$Month, bits_ca$Day, sep = "-"))

bits_ca$yday <- yday(bits_ca$Date)
```

### Now clean the CA data and go to 1 row 1 observation

```{r, message=FALSE}
bits_ca <- bits_ca %>% 
  filter(SpecCode %in% c("164712", "126436") & Year < 2020) %>% 
  mutate(Species = "Cod")

# Now I need to copy rows with NoAtLngt > 1 so that 1 row = 1 ind
# First make a small test
# nrow(bits_ca)
# test_id <- head(filter(bits_ca, CANoAtLngt == 5))$ID[1]
# filter(bits_ca, ID == test_id & CANoAtLngt == 5)

bits_ca <- bits_ca %>% map_df(., rep, .$CANoAtLngt)

# head(data.frame(filter(bits_ca, ID == test_id & CANoAtLngt == 5)), 20)
# nrow(bits_ca)
# Looks ok!

# Standardize length and drop NA weights (need that for condition)
bits_ca <- bits_ca %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_g = IndWgt) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length (https://vocab.ices.dk/?ref=18)

# Plot
ggplot(bits_ca, aes(IndWgt, length_cm)) +
  geom_point() + 
  facet_wrap(~Year)

# Remove apparent outlier
bits_ca <- bits_ca %>%
  mutate(keep = ifelse(Year == 2010 & IndWgt > 12500, "N", "Y")) %>% 
  filter(keep == "Y") %>% dplyr::select(-keep)

# bits_ca %>% 
#   mutate(fulton_k = (IndWgt / (length_cm^3)) * 100) %>% 
#   filter(fulton_k < 10) %>% 
#   ggplot(aes(yday, fulton_k)) + geom_point() + stat_smooth()

ggplot(bits_ca, aes(IndWgt, length_cm)) +
  geom_point() + 
  facet_wrap(~Year)

# Rename things and select specific columns
dat <- bits_ca %>% rename("year" = "Year",
                          "lat" = "ShootLat",
                          "lon" = "ShootLong",
                          "quarter" = "Quarter",
                          "ices_rect" = "Rect") %>% 
  dplyr::select(weight_g, length_cm, year, lat, lon, quarter, IDx, ices_rect, sub_div)

# > nrow(dat)
# [1] 98863
```

dat %>% drop_na(weight_g) %>% filter(year > 1992) %>% 
ggplot(., aes(lon, lat)) +
  geom_point(size = 0.1) + 
  facet_wrap(~year)

## Add in the environmental variables
### Depth

```{r}
# Read the tifs
west <- raster("data/depth_geo_tif/D5_2018_rgb-1.tif")
#plot(west)

east <- raster("data/depth_geo_tif/D6_2018_rgb-1.tif")
# plot(east)

dep_rast <- raster::merge(west, east)

dat$depth <- extract(dep_rast, dat[, 5:4])

# Convert to depth (instead of elevation)
ggplot(dat, aes(depth)) + geom_histogram()
dat$depth <- (dat$depth - max(drop_na(dat)$depth)) *-1
ggplot(dat, aes(depth)) + geom_histogram()
```

### Oxygen

```{r}
# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1610091357600.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

# We only use Quarter 4 in this analysis, so now we want to loop through each time step,
# and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep <- which(months > 9)

oxy_q4 <- oxy_array[, , index_keep]

months_keep <- months[index_keep]

years_keep <- years[index_keep]

# Now we have an array with only Q4 data...
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq <- seq(1, dim(oxy_q4)[3], by = 3)

# Create objects that will hold data
dlist <- list()
oxy_10 <- c()
oxy_11 <- c()
oxy_12 <- c()
oxy_ave <- c()

# Loop through the vector sequence with every third value, then take the average of
# three consecutive months (i.e. q4)
for(i in loop_seq) {
  
  oxy_10 <- oxy_q4[, , (i)]
  oxy_11 <- oxy_q4[, , (i + 1)]
  oxy_12 <- oxy_q4[, , (i + 2)]
  
  oxy_ave <- (oxy_10 + oxy_11 + oxy_12) / 3
  
  list_pos <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist[[list_pos]] <- oxy_ave
  
}

# Now name the lists with the year:
names(dlist) <- unique(years_keep)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat so far in this script

# Filter years in the cpue data frame to only have the years I have oxygen for
d_sub_oxy <- dat %>% filter(year %in% names(dlist)) %>% droplevels()

# Create data holding object
data_list <- list()

# ... And for the oxygen raster
raster_list <- list()

# Create factor year for indexing the list in the loop
d_sub_oxy$year_f <- as.factor(d_sub_oxy$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_oxy$year_f)) {
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22

  # Subset a year
  oxy_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(oxy_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  #plot(r, main = i)
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- d_sub_oxy %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)
  
  # Extract raster value (oxygen)
  rasValue <- raster::extract(r, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice$oxy <- rasValue
  
  # Add in which year
  d_slice$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index <- as.numeric(d_slice$year)[1] - 1992
  
  # Add each years' data in the list
  data_list[[index]] <- d_slice
  
  # Save to check each year is ok! First convert the raster to points for plotting
  # (so that we can use ggplot)
  map.p <- rasterToPoints(r)
  
  # Make the points a dataframe for ggplot
  df_rast <- data.frame(map.p)
  
  # Rename y-variable and add year
  df_rast <- df_rast %>% rename("oxy" = "layer") %>% mutate(year = i)
  
  # Add each years' raster data frame in the list
  raster_list[[index]] <- df_rast
  
  # Make appropriate column headings
  colnames(df_rast) <- c("Longitude", "Latitude", "oxy")
  
  # Now make the map
  ggplot(data = df_rast, aes(y = Latitude, x = Longitude)) +
    geom_raster(aes(fill = oxy)) +
    geom_point(data = d_slice, aes(x = lon, y = lat, fill = oxy),
               color = "black", size = 5, shape = 21) +
    theme_bw() +
    geom_sf(data = world, inherit.aes = F, size = 0.2) +
    coord_sf(xlim = c(xmin, xmax),
             ylim = c(ymin, ymax)) +
    scale_colour_gradientn(colours = rev(terrain.colors(10)),
                           limits = c(-200, 400)) +
    scale_fill_gradientn(colours = rev(terrain.colors(10)),
                         limits = c(-200, 400)) +
    NULL
  
  ggsave(paste("figures/supp/condition/cond_oxygen_rasters/", i,".png", sep = ""),
         width = 6.5, height = 6.5, dpi = 600)
  
}

# Now create a data frame from the list of all annual values
big_dat_oxy <- dplyr::bind_rows(data_list)
big_raster_dat_oxy <- dplyr::bind_rows(raster_list)

# Plot data, looks like there's big inter-annual variation but a negative trend over time
big_raster_dat_oxy %>%
  group_by(year) %>%
  drop_na(oxy) %>%
  summarise(mean_oxy = mean(oxy)) %>%
  mutate(year_num = as.numeric(year)) %>%
  ggplot(., aes(year_num, mean_oxy)) +
  geom_point(size = 2) +
  stat_smooth(method = "lm") +
  NULL

big_raster_dat_oxy %>%
  group_by(year) %>%
  drop_na(oxy) %>%
  mutate(dead = ifelse(oxy < 0, "Y", "N")) %>%
  filter(dead == "Y") %>%
  mutate(n = n(),
         year_num = as.numeric(year)) %>%
  ggplot(., aes(year_num, n)) +
  geom_point(size = 2) +
  stat_smooth(method = "lm") +
  NULL

# Now add in the new oxygen column in the original data:
str(d_sub_oxy)
str(big_dat_oxy)

# Create an ID for matching the oxygen data with the cpue data
dat$id_oxy <- paste(dat$year, dat$lon, dat$lat, sep = "_")
big_dat_oxy$id_oxy <- paste(big_dat_oxy$year, big_dat_oxy$lon, big_dat_oxy$lat, sep = "_")

# Which id's are not in the cpue data (dat)?
ids <- dat$id_oxy[!dat$id_oxy %in% c(big_dat_oxy$id_oxy)]

unique(ids)

# Select only the columns we want to merge
big_dat_sub_oxy <- big_dat_oxy %>% dplyr::select(id_oxy, oxy)

# Remove duplicate ID (one oxy value per id)
big_dat_sub_oxy2 <- big_dat_sub_oxy %>% distinct(id_oxy, .keep_all = TRUE)
# big_dat_sub_oxy %>% group_by(id_oxy) %>% mutate(n = n()) %>% arrange(desc(n))

# Join the data with raster-derived oxygen with the full cpue data
dat <- left_join(dat, big_dat_sub_oxy2, by = "id_oxy")

# Now the unit of oxygen is mmol/m3. I want it to be ml/L. The original model is in unit ml/L
# and it's been converted by the data host. Since it was converted without accounting for
# pressure or temperature, I can simply use the following conversion factor:
# 1 ml/l = 103/22.391 = 44.661 μmol/l -> 1 ml/l = 0.044661 mmol/l = 44.661 mmol/m^3 -> 0.0223909 ml/l = 1mmol/m^3
# https://ocean.ices.dk/tools/unitconversion.aspx

dat$oxy <- dat$oxy * 0.0223909

# Drop NA oxygen
dat <- dat %>% drop_na(oxy)
```

### Temperature

```{r}
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1608127623694.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get temperature
dname <- "bottomT"

temp_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(temp_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
temp_array[temp_array == fillvalue$value] <- NA

# We only use Quarter 4 in this analysis, so now we want to loop through each time step,
# and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep <- which(months > 9)

# Quarter 4 by keeping months in index_keep
temp_q4 <- temp_array[, , index_keep]

months_keep <- months[index_keep]

years_keep <- years[index_keep]

# Now we have an array with only Q4 data...
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq <- seq(1, dim(temp_q4)[3], by = 3)

# Create objects that will hold data
dlist <- list()
temp_10 <- c()
temp_11 <- c()
temp_12 <- c()
temp_ave <- c()

# Loop through the vector sequence with every third value, then take the average of
# three consecutive months (i.e. q4)
for(i in loop_seq) {
  
  temp_10 <- temp_q4[, , (i)]
  temp_11 <- temp_q4[, , (i + 1)]
  temp_12 <- temp_q4[, , (i + 2)]
  
  temp_ave <- (temp_10 + temp_11 + temp_12) / 3
  
  list_pos <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist[[list_pos]] <- temp_ave
  
}

# Now name the lists with the year:
names(dlist) <- unique(years_keep)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat so far in this script

# Filter years in the cpue data frame to only have the years I have temperature for
d_sub_temp <- dat %>% filter(year %in% names(dlist)) %>% droplevels()

# Create data holding object
data_list <- list()

# ... And for the temperature raster
raster_list <- list()

# Create factor year for indexing the list in the loop
d_sub_temp$year_f <- as.factor(d_sub_temp$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_temp$year_f)) {
  
  # Subset a year
  temp_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(temp_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  #plot(r, main = i)
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- d_sub_temp %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)
  
  # Extract raster value (temperature)
  rasValue <- raster::extract(r, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice$temp <- rasValue
  
  # Add in which year
  d_slice$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index <- as.numeric(d_slice$year)[1] - 1992
  
  # Add each years' data in the list
  data_list[[index]] <- d_slice
  
  # Save to check each year is ok! First convert the raster to points for plotting
  # (so that we can use ggplot)
  map.p <- rasterToPoints(r)
  
  # Make the points a dataframe for ggplot
  df_rast <- data.frame(map.p)
  
  # Rename y-variable and add year
  df_rast <- df_rast %>% rename("temp" = "layer") %>% mutate(year = i)
  
  # Add each years' raster data frame in the list
  raster_list[[index]] <- df_rast
  
  # Make appropriate column headings
  colnames(df_rast) <- c("Longitude", "Latitude", "temp")
  
  # Now make the map
  ggplot(data = df_rast, aes(y = Latitude, x = Longitude)) +
    geom_raster(aes(fill = temp)) +
    geom_point(data = d_slice, aes(x = lon, y = lat, fill = temp),
               color = "black", size = 5, shape = 21) +
    theme_bw() +
    geom_sf(data = world, inherit.aes = F, size = 0.2) +
    coord_sf(xlim = c(min(dat$lon), max(dat$lon)),
             ylim = c(min(dat$lat), max(dat$lat))) +
    scale_colour_gradientn(colours = rev(terrain.colors(10)),
                           limits = c(2, 17)) +
    scale_fill_gradientn(colours = rev(terrain.colors(10)),
                         limits = c(2, 17)) +
    NULL
  
  ggsave(paste("figures/supp/condition/cond_temp_rasters/", i,".png", sep = ""),
         width = 6.5, height = 6.5, dpi = 600)
  
}

# Now create a data frame from the list of all annual values
big_dat_temp <- dplyr::bind_rows(data_list)
big_raster_dat_temp <- dplyr::bind_rows(raster_list)

big_dat_temp %>% drop_na(temp) %>% summarise(max = max(temp))
big_dat_temp %>% drop_na(temp) %>% summarise(min = min(temp))

# Plot data, looks like there's big inter-annual variation but a positive trend
big_raster_dat_temp %>%
  group_by(year) %>%
  drop_na(temp) %>%
  summarise(mean_temp = mean(temp)) %>%
  mutate(year_num = as.numeric(year)) %>%
  ggplot(., aes(year_num, mean_temp)) +
  geom_point(size = 2) +
  stat_smooth(method = "lm") +
  NULL

# Now add in the new temperature column in the original data:
str(d_sub_temp)
str(big_dat_temp)

# Create an ID for matching the temperature data with the cpue data
dat$id_temp <- paste(dat$year, dat$lon, dat$lat, sep = "_")
big_dat_temp$id_temp <- paste(big_dat_temp$year, big_dat_temp$lon, big_dat_temp$lat, sep = "_")

# Which id's are not in the cpue data (dat)? (It's because I don't have those years, not about the location)
ids <- dat$id_temp[!dat$id_temp %in% c(big_dat_temp$id_temp)]

unique(ids)

# Select only the columns we want to merge
big_dat_sub_temp <- big_dat_temp %>% dplyr::select(id_temp, temp)

# Remove duplicate ID (one temp value per id)
big_dat_sub_temp2 <- big_dat_sub_temp %>% distinct(id_temp, .keep_all = TRUE)

# Join the data with raster-derived oxygen with the full cpue data
dat <- left_join(dat, big_dat_sub_temp2, by = "id_temp")

colnames(dat)

dat <- dat %>% dplyr::select(-id_temp, -id_oxy)

# Drop NA temp
dat <- dat %>% drop_na(temp)
```

## Add UTM coords

```{r}
# First add UTM coords
# Add UTM coords
# Function
LongLatToUTM <- function(x, y, zone){
  xy <- data.frame(ID = 1:length(x), X = x, Y = y)
  coordinates(xy) <- c("X", "Y")
  proj4string(xy) <- CRS("+proj=longlat +datum=WGS84")  ## for example
  res <- spTransform(xy, CRS(paste("+proj=utm +zone=",zone," ellps=WGS84",sep='')))
  return(as.data.frame(res))
}

utm_coords <- LongLatToUTM(dat$lon, dat$lat, zone = 33)
dat$X <- utm_coords$X/1000 # for computational reasons
dat$Y <- utm_coords$Y/1000 # for computational reasons
```

## Add cod and flounder density covariates

```{r, message=FALSE}
# This is so that we can standardize the prediction grid with respect to the data
density <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/catch_q_1_4.csv")

density <- density %>% filter(quarter == 4)

# Load models
mcod <- readRDS("output/mcod.rds")
mfle <- readRDS("output/mfle.rds")

# We need to scale data with respect to the mean and sd in the data to it was fitted to (cpue data)
density_mean_depth <- mean(density$depth)
density_sd_depth <- sd(density$depth)

# Scale depth in the data
dat <- dat %>%
  mutate(depth_sc = (depth - density_mean_depth) / density_sd_depth)

hist((density$depth - mean(density$depth)) / sd(density$depth))
hist(dat$depth_sc)

# Predict from the density models
cpue_cod <- exp(predict(mcod, newdata = dat)$est)
cpue_fle <- exp(predict(mfle, newdata = dat)$est)

dat$density_cod <- cpue_cod
dat$density_fle <- cpue_fle

# Inspect
ggplot(dat, aes(log(density_cod))) + geom_histogram()
ggplot(dat, aes(log(density_fle))) + geom_histogram()

# Remove the scaled depth variable
dat <- dat %>% dplyr::select(-depth_sc)
```

### ... And secondly by doing a `left_join` from the CPUE data using `haul.id` 

```{r, message=FALSE}
d_cod_select <- density %>% dplyr::select(density, IDx) %>% rename("density_cod_data" = "density")
d_fle_select <- density %>% dplyr::select(density_fle, IDx) %>% rename("density_fle_data" = "density_fle")

dat <- left_join(dat, d_cod_select)
dat <- left_join(dat, d_fle_select)

# Check how well they correspond
ggplot(dat, aes(density_cod_data, density_cod)) + geom_point() + geom_abline(color = "red")
ggplot(dat, aes(density_fle_data, density_fle)) + geom_point() + geom_abline(color = "red")
```

## Add Saduria from raster covariates

```{r read Saduria raster from Gogina et al 2020 ICES}
saduria <- raster("data/saduria_tif/FWBiomassm_raster_19812019presHighweightcor_no0_newZi.tif")

saduria_longlat = projectRaster(saduria, crs = ('+proj=longlat'))

crs(saduria)
extent(saduria)

plot(saduria_longlat)

# Now extract the values from the saduria raster to the data
dat$density_saduria <- extract(saduria_longlat, dat[, 5:4])

ggplot(dat, aes(lon, lat, color = density_saduria)) +
  geom_point()
```

## Add large scale variables from the prediciton grid

```{r add large scale variables}
# First explore what the difference is between filtering depths and oxygen within their niche rather than the total in the rectangle

pred_grid1 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/pred_grid_(1_2).csv")
pred_grid2 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/pred_grid_(2_2).csv")

pred_grid <- bind_rows(pred_grid1, pred_grid2)
 
unique(is.na(pred_grid$density_cod))
unique(is.na(pred_grid$density_cod_rec))

# First create ID's for joining
dat <- dat %>% mutate(year_rect_id = paste(year, ices_rect, sep = "_"),
                      year_sd_id = paste(year, sub_div, sep = "_"))

# t <- pred_grid %>% 
#   mutate(test_id = paste(year, ices_rect)) %>% 
#   group_by(test_id) %>% 
#   mutate(n = unique(temp_rec)) %>% 
#   ungroup() %>% 
#   distinct(n)
# 
# tt <- pred_grid %>% filter(year == "1994" & ices_rect == "39G7")
# unique(tt$temp_rec)

# Ok, some weird rounding error! Anyhow... 

# Take distinct year + ices_rect id's and select the variables
pred_grid_rec_vars <- pred_grid %>% 
  mutate(year_rect_id = paste(year, ices_rect, sep = "_")) %>% 
  distinct(year_rect_id, .keep_all = TRUE) %>% 
  dplyr::select(year_rect_id,
                density_saduria_rec, density_cod_rec, density_fle_rec,
                depth_rec, oxy_rec, temp_rec, biomass_spr, biomass_her)

unique(is.na(pred_grid_rec_vars$density_cod_rec))  

# Join!
dat <- left_join(dat, pred_grid_rec_vars)

unique(is.na(dat$density_cod_rec))  

# t <- dat %>% drop_na(density_cod_rec)
# t <- dat %>% drop_na(biomass_her)
# t <- dat %>% drop_na(density_saduria_rec, density_cod_rec, density_fle_rec,
#                      depth_rec, oxy_rec, temp_rec, biomass_spr, biomass_her)

# Take distinct year + sub_div id's and select the variables
pred_grid_sd_vars <- pred_grid %>% 
  mutate(year_sd_id = paste(year, sub_div, sep = "_")) %>% 
  distinct(year_sd_id, .keep_all = TRUE) %>% 
  dplyr::select(year_sd_id,
                density_saduria_sd, density_cod_sd, density_fle_sd,
                depth_sd, oxy_sd, temp_sd, biomass_spr_sd, biomass_her_sd)

# Join!
dat <- left_join(dat, pred_grid_sd_vars)

t <- dat %>% drop_na(density_cod_rec)
t <- dat %>% drop_na(biomass_her)
t <- dat %>% drop_na(density_saduria_rec, density_cod_rec, density_fle_rec,
                     depth_rec, oxy_rec, temp_rec, biomass_spr, biomass_her,
                     density_saduria_sd, density_cod_sd, density_fle_sd, 
                     depth_sd, oxy_sd, temp_sd, biomass_spr_sd, biomass_her_sd)
```

## Save data

```{r, message=FALSE}
colnames(dat)

# Check for outliers
dat <- dat %>% 
  mutate(Fulton_K = weight_g/(0.01*length_cm^3)) %>% 
  filter(Fulton_K < 3 & Fulton_K > 0.15) # Visual exploration, larger values likely data entry errors

# Trim data a little bit to not make it unnecessarily large
dat <- dat %>%
  dplyr::select(-IDx, -quarter, -IDx, -year_sd_id, -year_rect_id, -Fulton_K)

dat_93_06 <- dat %>% filter(year < 2007)
dat_07_19 <- dat %>% filter(year > 2006)

write.csv(dat_93_06, file = "data/for_analysis/mdat_cond_(1_2).csv", row.names = FALSE)
write.csv(dat_07_19, file = "data/for_analysis/mdat_cond_(2_2).csv", row.names = FALSE)
```
                           
```{r}
knitr::knit_exit()
```

```{r test different ways to calculate ices_rectangle means}
niche_rect <- pred_grid %>%
  filter(depth < 100 & oxy > 1.5) %>%
  group_by(ices_rect, year) %>%
  summarise(median_depth = median(depth),
            median_oxy = median(oxy)) %>%
  mutate(year_rect_id = paste(year, ices_rect, sep = "_"),
         type = "niche_data") %>%
  ungroup()

all_rect <- pred_grid %>%
  group_by(ices_rect, year) %>%
  summarise(median_depth = median(depth),
            median_oxy = median(oxy)) %>%
  mutate(year_rect_id = paste(year, ices_rect, sep = "_"),
         type = "all_data") %>%
  ungroup()

all_dat <- bind_rows(all_rect, niche_rect)

# Now add in the condition data to calculate sample size per rectangle
# Filter
cond1 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/mdat_cond_(1_2).csv")
cond2 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/mdat_cond_(2_2).csv")

cond <- bind_rows(cond1, cond2) %>% mutate(year_rect_id = paste(year, ices_rect, sep = "_"))

# Join data for lm
all_rect2 <- all_rect %>% dplyr::select(year_rect_id, median_depth) %>%
  mutate(median_depth_sc = (median_depth - mean(median_depth))/sd(median_depth))

niche_rect2 <- niche_rect %>% dplyr::select(year_rect_id, median_depth) %>%
  mutate(median_depth_sc = (median_depth - mean(median_depth))/sd(median_depth))

cond_all <- left_join(cond, all_rect2)
cond_niche <- left_join(cond, niche_rect2)

summary(lm(log(weight_g) ~ log(length_cm) + median_depth_sc, data = cond_all))
summary(lm(log(weight_g) ~ log(length_cm) + median_depth_sc, data = cond_niche))

t <- cond_all %>% filter(median_depth < 100)

plot(cond_all$median_depth_sc)
plot(cond_niche$median_depth_sc)

cond2 <- cond %>%
  group_by(year, ices_rect) %>%
  summarise(n = n()) %>%
  mutate(year_rect_id = paste(year, ices_rect, sep = "_")) %>%
  ungroup() %>%
  dplyr::select(-year, -ices_rect)

cond2 %>% arrange(n) %>% as.data.frame()

all_dat <- left_join(all_dat, cond2)

hist(all_dat$n)

all_dat %>% arrange(n) %>% as.data.frame()

all_dat <- all_dat %>%
  drop_na(n) %>%
  mutate(small_sample = ifelse(n < 30, "Y", "N"))# %>% filter(small_sample == "N")

# Depth
ggplot(all_dat %>% filter(year == 1999)) +
  geom_bar(aes(ices_rect, median_depth, fill = type), position="dodge", stat="identity") +
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~small_sample, ncol = 1) +
  NULL

# Oxygen
ggplot(all_dat %>% filter(year == 1999)) +
  geom_bar(aes(ices_rect, median_oxy, fill = type), position="dodge", stat="identity") +
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~small_sample, ncol = 1) +
  NULL

# The fine scale variables are matched to haul. However, the ices_rect level variables
# and sub division averages should not be averages of the hauls in those areas, but rather
# average prediction on those levels. To achieve that, I will join in those averages from the pred_grid
```

```{r test effect of cod and flounder data or model}
d_test <- drop_na(dat) %>%
  mutate(ln_length_cm = log(length_cm),
         ln_weight_g = log(weight_g),
         year_ct = year - mean(year),
         biomass_her_sc = biomass_her,
         biomass_her_sd_sc = biomass_her_sd,
         biomass_spr_sc = biomass_spr,
         biomass_spr_sd_sc = biomass_spr_sd,
         density_cod_sc = density_cod,
         density_cod_data_sc = density_cod_data,
         density_cod_rec_sc = density_cod_rec,
         density_fle_sc = density_fle,
         density_fle_data_sc = density_fle_data,
         density_fle_rec_sc = density_fle_rec,
         density_saduria_sc = density_saduria,
         density_saduria_rec_sc = density_saduria_rec,
         depth_sc = depth,
         depth_rec_sc = depth_rec,
         depth_sd_sc = depth_sd,
         oxy_sc = oxy,
         oxy_rec_sc = oxy_rec,
         oxy_sd_sc = oxy_sd,
         temp_sc = temp,
         temp_rec_sc = temp_rec,
         temp_sd_sc = temp_sd)  %>%
  mutate_at(c("biomass_her_sc", "biomass_her_sd_sc", "biomass_spr_sc", "biomass_spr_sd_sc",
              "density_cod_sc", "density_cod_data_sc", "density_cod_rec_sc", 
              "density_fle_sc", "density_fle_data_sc", "density_fle_rec_sc", 
              "density_saduria_sc", "density_saduria_rec_sc", 
              "depth_sc", "depth_rec_sc", "depth_sd_sc",
              "oxy_sc", "oxy_rec_sc", "oxy_sd_sc",
              "temp_sc", "temp_rec_sc", "temp_sd_sc"
              ),
            ~(scale(.) %>% as.vector)) %>% 
  mutate(year = as.integer(year))


m1 <- lm(ln_weight_g ~ as.factor(year) + ln_length_cm + biomass_her_sc + biomass_her_sd_sc +
    biomass_spr_sc + biomass_spr_sd_sc +
    density_cod_sc + 
    density_fle_sc + 
    density_saduria_sc + density_saduria_rec_sc + 
    depth_sc + depth_rec_sc + 
    oxy_sc + oxy_rec_sc +
    temp_sc + temp_rec_sc,
    data = d_test)

summary(m1)

# density_cod_sc         -0.0038978  0.0005076   -7.678 1.63e-14 ***
# density_fle_sc          0.0015849  0.0005212    3.041 0.002362 ** 

m2 <- lm(ln_weight_g ~ as.factor(year) + ln_length_cm + biomass_her_sc + biomass_her_sd_sc +
    biomass_spr_sc + biomass_spr_sd_sc +
    density_cod_data_sc + 
    density_fle_data_sc + 
    density_saduria_sc + density_saduria_rec_sc + 
    depth_sc + depth_rec_sc + 
    oxy_sc + oxy_rec_sc +
    temp_sc + temp_rec_sc,
    data = d_test)

summary(m2)

# density_cod_data_sc    -0.0021723  0.0004714   -4.608 4.07e-06 ***
# density_fle_data_sc     0.0009110  0.0004950    1.840 0.065723 .  
```



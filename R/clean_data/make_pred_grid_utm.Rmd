---
title: "Make the prediction grid"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  df_print: paged
pdf_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.asp = 0.618,
  fig.align ='center'
)
```

# Intro
Make an evenly spaced UTM prediction grid with all spatially varying covariates. Requires running the script `cod_fle_density_models_as_covars` first.

```{r lib, message=FALSE}
# Load libraries, install if needed
library(tidyverse); theme_set(theme_light(base_size = 12))
library(tidylog)
library(viridis)
library(mapdata)
library(rgdal)
library(raster)
library(sf)
library(sp)
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(patchwork)
library(ncdf4)
library(chron)
library(gganimate)
library(gifski)
library(png)
library(RCurl)
library(sdmTMB)
library(RColorBrewer)
library(terra)
library(mapplots)
library(readxl)
```

## Basic grid with depth

```{r basic grid with depth, message = FALSE}
# These ranges are based on the condition data. Convert these to UTM!
ymin = 54
ymax = 58
xmin = 12
xmax = 22

# These are the years we are using
cond_years <- c(1993:2019)

# Function to go from lat long to UTM
LongLatToUTM <- function(x, y, zone){
  xy <- data.frame(ID = 1:length(x), X = x, Y = y)
  coordinates(xy) <- c("X", "Y")
  proj4string(xy) <- CRS("+proj=longlat +datum=WGS84")  ## for example
  res <- spTransform(xy, CRS(paste("+proj=utm +zone=",zone," ellps=WGS84",sep='')))
  return(as.data.frame(res))
}

LongLatToUTM(11, 53, 33)
LongLatToUTM(11, 58, 33)
LongLatToUTM(22, 53, 33)
LongLatToUTM(22, 58, 33)

# Round values based on above
utm_x_min <- 230000
utm_x_max <- 960000
utm_y_min <- 5900000
utm_y_max <- 6450000

# Make the evenly spaced (on UTM) grid 
pred_grid <- expand.grid(
  X = seq(utm_x_min, utm_x_max, by = 4000),
  Y = seq(utm_y_min, utm_y_max, by = 4000)) # 4x4 km

# For adding maps to plots
world <- ne_countries(scale = "medium", returnclass = "sf")

map_data <- rnaturalearth::ne_countries(
  scale = "medium",
  returnclass = "sf", continent = "europe")

swe_coast <- suppressWarnings(suppressMessages(
  st_crop(map_data,
          c(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax))))

# Transform our map into UTM 9 coordinates, which is the equal-area projection we fit in:
utm_zone33 <- 32633
swe_coast_proj <- sf::st_transform(swe_coast, crs = utm_zone33)

ggplot(swe_coast_proj) + geom_sf()

ggplot(swe_coast_proj) + geom_sf() +
  geom_point(data = pred_grid, aes(x = X, y = Y), alpha = 0.1, shape = 21, fill = NA) +
  theme_light() +
  labs(x = "Longitude", y = "Latitude")

# Looks OK!

# Now we need to add depth
west <- raster("data/depth_geo_tif/D5_2018_rgb-1.tif")
plot(west)

east <- raster("data/depth_geo_tif/D6_2018_rgb-1.tif")
plot(east)

dep_rast <- raster::merge(west, east)

plot(dep_rast)

# Do that by extracting depths of the pred grid
utm_coords <- pred_grid %>% dplyr::select(X, Y)

# Reproject the raster to fit the UTM pred grid...
# Define spatial reference 
sr <- "+proj=utm +zone=33  +datum=WGS84 +units=m " 

# Project Raster... This takes some time
projected_raster <- projectRaster(dep_rast, crs = sr)

utm_coords$depth <- extract(projected_raster, utm_coords[, 1:2])
max(utm_coords$depth)
min(utm_coords$depth)

hist(utm_coords$depth)

# Convert to depth (instead of elevation)
ggplot(utm_coords, aes(depth)) + geom_histogram()
utm_coords$depth <- utm_coords$depth - max(utm_coords$depth)
ggplot(utm_coords, aes(depth)) + geom_histogram()

ggplot(swe_coast_proj) + geom_sf() +
  geom_point(data = filter(utm_coords, depth < 0), aes(x = X, y = Y, color = depth)) +
  theme_light() +
  scale_colour_viridis() + 
  labs(x = "Longitude", y = "Latitude")

df <- utm_coords %>% filter(depth < 0) %>% mutate(depth = depth*-1)

# Now make a new grid
pred_grid <- data.frame(X = rep(df$X, length(unique(cond_years))),
                        Y = rep(df$Y, length(unique(cond_years))),
                        depth = rep(df$depth, length(unique(cond_years))),
                        year = rep(sort(unique(cond_years)), each = nrow(df)))

pred_grid <- pred_grid %>% mutate(deep = ifelse(depth > 135, "Y", "N"))

ggplot(swe_coast_proj) + 
  geom_raster(data = filter(pred_grid, year == "1999"), aes(x = X, y = Y, fill = deep)) +
  geom_sf() +
  theme_light() +
  labs(x = "Longitude", y = "Latitude")

ggplot(swe_coast_proj) + 
  geom_raster(data = filter(pred_grid, year == "1999"), aes(x = X, y = Y, fill = depth)) +
  geom_sf() +
  theme_light() +
  labs(x = "Longitude", y = "Latitude")

hist(pred_grid$depth)
```

## Oxygen and temperature from rasters

```{r oxygen and temperature }
# ** Oxygen ========================================================================
# Loop through each year and extract the oxygen levels
# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1610091357600.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

# We only use Quarter 4 in this analysis, so now we want to loop through each time step,
# and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep <- which(months > 9)

# Quarter 4 by keeping months in index_keep
oxy_q4 <- oxy_array[, , index_keep]

months_keep <- months[index_keep]

years_keep <- years[index_keep]

# Now we have an array with only Q4 data...
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq <- seq(1, dim(oxy_q4)[3], by = 3)

# Create objects that will hold data
dlist <- list()
oxy_10 <- c()
oxy_11 <- c()
oxy_12 <- c()
oxy_ave <- c()

# Loop through the vector sequence with every third value, then take the average of
# three consecutive months (i.e. q4)
for(i in loop_seq) {
  
  oxy_10 <- oxy_q4[, , (i)]
  oxy_11 <- oxy_q4[, , (i + 1)]
  oxy_12 <- oxy_q4[, , (i + 2)]
  
  oxy_ave <- (oxy_10 + oxy_11 + oxy_12) / 3
  
  list_pos <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist[[list_pos]] <- oxy_ave
  
}

# Now name the lists with the year:
names(dlist) <- unique(years_keep)

# Now I need to make a loop where I extract the raster value for each year...

# Filter years in the pred-grid data frame to only have the years I have oxygen for
d_sub_oxy <- pred_grid %>% filter(year %in% names(dlist)) %>% droplevels()

# Create data holding object
data_list <- list()

# Create factor year for indexing the list in the loop
d_sub_oxy$year_f <- as.factor(d_sub_oxy$year)

# Loop through each year and extract raster values for the pred-grid data points
for(i in unique(d_sub_oxy$year_f)) {
  
  # Subset a year
  oxy_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(oxy_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  plot(r, main = i)
  
  # Change projection to UTM (same as pred grid)
  proj_raster <- projectRaster(r, crs = sr)
  
  # Filter the same year (i) in the pred-grid data and select only coordinates
  d_slice <- d_sub_oxy %>% filter(year_f == i) %>% dplyr::select(X, Y)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice, proj4string = CRS(sr))
  
  # Extract raster value (oxygen)
  rasValue <- raster::extract(proj_raster, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the pred-grid
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the pred-grid data
  d_slice$oxy <- rasValue
  
  # Add in which year
  d_slice$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index <- as.numeric(d_slice$year)[1] - 1992
  
  # Add each years' data in the list
  data_list[[index]] <- d_slice
  
}

# Now create a data frame from the list of all annual values
pred_grid_oxy <- dplyr::bind_rows(data_list)

lims <- pred_grid_oxy %>% drop_na(oxy) %>% summarise(min = min(oxy),
                                                     max = max(oxy))
# Plot and compare with rasters
ggplot(swe_coast_proj) +
  geom_raster(data = pred_grid_oxy,
             aes(x = X, y = Y, fill = oxy)) +
  geom_sf() +
  scale_fill_gradientn(colours = rev(terrain.colors(10)),
                         limits = c(lims$min, lims$max)) +
  theme_light() +
  facet_wrap(~ year) + 
  labs(fill = "Oxygen") +
  labs(x = "Longitude", y = "Latitude")

# Animation
# pred_grid_oxy$year <- as.integer(pred_grid_oxy$year)
# 
# p <- ggplot(swe_coast_proj) + geom_sf() +
#   geom_raster(data = pred_grid_oxy,
#               aes(x = X, y = Y, fill = oxy)) +
#   scale_fill_gradientn(colours = rev(terrain.colors(10)),
#                        limits = c(lims$min, lims$max)) +
#   theme_light() +
#   labs(fill = "Oxygen") +
#   labs(x = "Longitude", y = "Latitude")
# 
# # Here comes the gganimate specific bits
# anim <- p +
#   labs(title = 'Year: {frame_time}') +
#   transition_time(as.integer(year)) +
#   ease_aes('linear')
# 
# gganimate::animate(anim, height = 600, width = 600)
# 
# anim_save(filename = "output/gif/oxy.gif")

# Add in oxygen to the main prediction grid
pred_grid_oxy <- pred_grid_oxy %>% arrange(X, Y, year)
pred_grid <- pred_grid %>% arrange(X, Y, year)

str(pred_grid_oxy)
str(pred_grid)

pred_grid$oxy <- pred_grid_oxy$oxy

# Now the unit of oxygen is mmol/m3. I want it to be ml/L. The original model is in unit ml/L
# and it's been converted by the data host. Since it was converted without accounting for
# pressure or temperature, I can simply use the following conversion factor:
# From Ye: 1 ml/l = 10^3/22.391 = 44.661 μmol/l (same as mmol/m^3)
# Hence, 0.0223909 ml/l = 1 μmol/l (mmol/m^3)

pred_grid$oxy <- pred_grid$oxy * 0.0223909


# ** Temperature ===================================================================
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1608127623694.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get temperature
dname <- "bottomT"

temp_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(temp_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
temp_array[temp_array == fillvalue$value] <- NA

# We only use Quarter 4 in this analysis, so now we want to loop through each time step,
# and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep <- which(months > 9)

# Quarter 4 by keeping months in index_keep
temp_q4 <- temp_array[, , index_keep]

months_keep <- months[index_keep]

years_keep <- years[index_keep]

# Now we have an array with only Q4 data...
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq <- seq(1, dim(temp_q4)[3], by = 3)

# Create objects that will hold data
dlist <- list()
temp_10 <- c()
temp_11 <- c()
temp_12 <- c()
temp_ave <- c()

# Loop through the vector sequence with every third value, then take the average of
# three consecutive months (i.e. q4)
for(i in loop_seq) {
  
  temp_10 <- temp_q4[, , (i)]
  temp_11 <- temp_q4[, , (i + 1)]
  temp_12 <- temp_q4[, , (i + 2)]
  
  temp_ave <- (temp_10 + temp_11 + temp_12) / 3
  
  list_pos <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist[[list_pos]] <- temp_ave
  
}

# Now name the lists with the year:
names(dlist) <- unique(years_keep)

# Now I need to make a loop where I extract the raster value for each year...

# Filter years in the pred-grid data frame to only have the years I have temperature for
d_sub_temp <- pred_grid %>% filter(year %in% names(dlist)) %>% droplevels()

# Create data holding object
data_list <- list()

# Create factor year for indexing the list in the loop
d_sub_temp$year_f <- as.factor(d_sub_temp$year)

# Loop through each year and extract raster values for the pred-grid data points
for(i in unique(d_sub_temp$year_f)) {
  
  # Subset a year
  temp_slice <- dlist[[i]]
  
  # Create raster for that year (i)
  r <- raster(t(temp_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r <- flip(r, direction = 'y')
  
  plot(r, main = i)
  
  proj_raster <- projectRaster(r, crs = sr)
  
  # Filter the same year (i) in the pred-grid data and select only coordinates
  d_slice <- d_sub_temp %>% filter(year_f == i) %>% dplyr::select(X, Y)
  
  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice, proj4string = CRS(sr))
  
  # Extract raster value (temperature)
  rasValue <- raster::extract(proj_raster, data_sp)
  
  # Now we want to plot the results of the raster extractions by plotting the pred-grid
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df <- as.data.frame(data_sp)
  
  # Add in the raster value in the df holding the coordinates for the pred-grid data
  d_slice$temp <- rasValue
  
  # Add in which year
  d_slice$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index <- as.numeric(d_slice$year)[1] - 1992
  
  # Add each years' data in the list
  data_list[[index]] <- d_slice
  
}

# Now create a data frame from the list of all annual values
pred_grid_temp <- dplyr::bind_rows(data_list)

lims <- pred_grid_temp %>% drop_na(temp) %>% summarise(min = min(temp),
                                                       max = max(temp))

# Plot and compare with rasters
ggplot(swe_coast_proj) +
  geom_raster(data = pred_grid_temp,
              aes(x = X, y = Y, fill = temp)) +
  geom_sf() +
  scale_fill_gradientn(colours = rev(terrain.colors(10)),
                       limits = c(lims$min, lims$max)) +
  theme_light() +
  facet_wrap(~ year) + 
  labs(fill = "Temperature") +
  labs(x = "Longitude", y = "Latitude")

# Animation
# pred_grid_temp$year <- as.integer(pred_grid_temp$year)
# 
# p <- ggplot(swe_coast_proj) + geom_sf() +
#   geom_raster(data = pred_grid_temp,
#               aes(x = X, y = Y, fill = temp)) +
#   scale_fill_gradientn(colours = rev(terrain.colors(10)),
#                        limits = c(lims$min, lims$max)) +
#   theme_light() +
#   labs(fill = "Temperature") +
#   labs(x = "Longitude", y = "Latitude")
# 
# # Here comes the gganimate specific bits
# anim <- p +
#   labs(title = 'Year: {frame_time}') +
#   transition_time(as.integer(year)) +
#   ease_aes('linear')
# 
# gganimate::animate(anim, height = 600, width = 600)
# 
# anim_save(filename = "output/gif/temp.gif")

# Add in temperature to the main prediction grid
pred_grid_temp <- pred_grid_temp %>% arrange(X, Y, year)
pred_grid <- pred_grid %>% arrange(X, Y, year)

str(pred_grid_temp)
str(pred_grid)

pred_grid$temp <- pred_grid_temp$temp
```

## Latitude and longitude coordinates to easily add in ICES information

```{r lat long, message=FALSE}
# Need to go from UTM to lat long for this one... 
# https://stackoverflow.com/questions/30018098/how-to-convert-utm-coordinates-to-lat-and-long-in-r
xy <- as.matrix(pred_grid[, 1:2])
v <- vect(xy, crs="+proj=utm +zone=33 +datum=WGS84  +units=m")
y <- project(v, "+proj=longlat +datum=WGS84")
lonlat <- geom(y)[, c("x", "y")]

pred_grid$lon <- lonlat[, 1]
pred_grid$lat <- lonlat[, 2]

# Also do UTM in km for computational speed
pred_grid$X <- pred_grid$X/1000
pred_grid$Y <- pred_grid$Y/1000
```

## Add ICES areas

```{r ices areas}
# https://stackoverflow.com/questions/34272309/extract-shapefile-value-to-point-with-r
# https://gis.ices.dk/sf/
shape <- shapefile("data/ICES_StatRec_mapto_ICES_Areas/StatRec_map_Areas_Full_20170124.shp")
head(shape)

plot(shape, axes = TRUE)

pts <- SpatialPoints(cbind(pred_grid$lon, pred_grid$lat), 
                     proj4string = CRS(proj4string(shape)))

pred_grid$subdiv <- over(pts, shape)$Area_27
pred_grid$subdiv2 <- over(pts, shape)$AreasList

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(pred_grid$subdiv))

pred_grid <- pred_grid %>% 
  mutate(sub_div = factor(subdiv),
         sub_div = fct_recode(subdiv,
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2"),
         sub_div = as.character(sub_div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  filter(lat > 54 & lat < 58 & lon < 22)

# Add ICES rectangles
pred_grid$ices_rect <- mapplots::ices.rect2(lon = pred_grid$lon, lat = pred_grid$lat)
```

## Saduria biomass densities

```{r, saduria densities, message=FALSE}
saduria <- raster("data/saduria_tif/FWBiomassm_raster_19812019presHighweightcor_no0_newZi.tif")
saduria_longlat = projectRaster(saduria, crs = ('+proj=longlat'))

# Now extract the values from the saduria raster to the prediction grid
pred_grid$density_saduria <- extract(saduria_longlat, pred_grid[, 8:9])

# Finest scale
ggplot(pred_grid, aes(X, Y, fill = density_saduria)) + 
  geom_raster()
```

## Sprat and herring biomasses

```{r sprat and herring, message=FALSE}
theme_facet_map <- function(base_size = 10, base_family = "") {
  theme_light(base_size = 10, base_family = "") +
    theme(
        axis.text.x = element_text(angle = 90),
        axis.text = element_text(size = 6),
        strip.text = element_text(size = 8, colour = 'gray10', margin = margin()),
        strip.background = element_rect(fill = "gray95"),
        legend.position = c(0.7, 0.02),
        legend.direction = "horizontal"
      )
}

# Read data on rectangle level
spr <- read_xlsx("data/BIAS/N and B per Rect. 1991-2020.xlsx",
                 sheet = 4) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("ices_rect" = "RECT",
         "Year" = "ANNUS") %>%
  mutate_at(vars(`1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(ices_rect = as.factor(ices_rect),
         Species = "Sprat",
         biomass_spr = `1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`, 
         IDr = paste(ices_rect, Year, sep = ".")) # Make new ID

her <- read_xlsx("data/BIAS/N and B per Rect. 1991-2020.xlsx",
                 sheet = 3) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("ices_rect" = "RECT",
         "Year" = "ANNUS") %>%
  mutate_at(vars(`1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`), ~replace_na(., 0)) %>%
  mutate(ices_rect = as.factor(ices_rect),
         Species = "Herring",
         biomass_her = `1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`, 
         IDr = paste(ices_rect, Year, sep = ".")) # Make new ID)

# Plot distribution over time in the whole area
spr %>%
  mutate(lon = ices.rect(spr$ices_rect)$lon) %>%
  mutate(lat = ices.rect(spr$ices_rect)$lat) %>%
  filter(Year > 1992 & Year < 2020) %>% 
  ggplot(., aes(lon, lat, fill = log(biomass_spr))) +
  geom_raster() +
  scale_fill_viridis() +
  facet_wrap(~ Year, ncol = 5) +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  labs(x = "lon", y = "lat") +
  theme_facet_map() + 
  NULL

ggsave("figures/supp/spr_distribution.png", width = 10, height = 10, dpi = 600)

her %>%
  mutate(lon = ices.rect(her$ices_rect)$lon) %>%
  mutate(lat = ices.rect(her$ices_rect)$lat) %>%
  filter(Year > 1992 & Year < 2020) %>% 
  ggplot(., aes(lon, lat, fill = log(biomass_her))) +
  geom_raster() +
  scale_fill_viridis() +
  facet_wrap(~ Year, ncol = 5) +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  labs(x = "lon", y = "lat") +
  theme_facet_map() + 
  NULL

ggsave("figures/supp/her_distribution.png", width = 10, height = 10, dpi = 600)

# How many unique rows per IDr?
her %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ggplot(., aes(factor(n))) + geom_bar()

spr %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ggplot(., aes(factor(n))) + geom_bar()

# Ok, some ID's with two rows...
spr %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  filter(n == 2) %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  head(20)

# It's because rectangles somehow being in different sub divisions.
# I need to group by IDr and summarize
spr_sum <- spr %>%
  group_by(IDr) %>% 
  summarise(biomass_spr = sum(biomass_spr)) %>% # Sum abundance within IDr
  distinct(IDr, .keep_all = TRUE) %>% # Remove duplicate IDr
  mutate(ID_temp = IDr) %>% # Create temporary IDr that we can use to split in order
  # to get Year and StatRect back into the summarized data
  separate(ID_temp, c("StatRec", "Year"), sep = 4)

nrow(spr_sum) 
nrow(spr)
nrow(spr %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2))

# Check with a specific rectangle
filter(spr_sum, IDr == "39G4.1991")
filter(spr, IDr == "39G4.1991")

# This should equal 1 (new # rows =  old - duplicated IDr)
nrow(spr_sum) / (nrow(spr) - 0.5*nrow(spr %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2)))

# How many rows per rectangle?
spr_sum %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  distinct(n)

# Now do the same for herring
her_sum <- her %>%
  group_by(IDr) %>% 
  summarise(biomass_her = sum(biomass_her)) %>% # Sum abundance within IDr
  distinct(IDr, .keep_all = TRUE) %>% # Remove duplicate IDr
  mutate(ID_temp = IDr) %>% # Create temporary IDr that we can use to split in order
  # to get Year and StatRect back into the summarized data
  separate(ID_temp, c("StatRec", "Year"), sep = 4)

nrow(her_sum) 
nrow(her)
nrow(her %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2))

filter(her_sum, IDr == "39G4.1991")
filter(her, IDr == "39G4.1991")

# This should equal 1 (new # rows =  old - duplicated IDr)
nrow(her_sum) / (nrow(her) - 0.5*nrow(her %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2)))

# How many rows per rectangle?
her_sum %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  distinct(n)

# Now join pelagic covariates with pred grid
# Make ices_rect a factor in the main data
pred_grid <- pred_grid %>% mutate(ices_rect = as.factor(ices_rect))

# Create IDr in prediction grid to match pelagics data
pred_grid <- pred_grid %>% mutate(IDr = paste(ices_rect, year, sep = "."))

# Are there any StatRec that are in the prediction grid data that are not in the pelagics data?
# Some very coastal...
unique(pred_grid$ices_rect[!pred_grid$ices_rect %in% her$ices_rect])
unique(pred_grid$ices_rect[!pred_grid$ices_rect %in% spr$ices_rect])

# Check IDr
pred_grid$IDr[!pred_grid$IDr %in% her$IDr]
pred_grid$IDr[!pred_grid$IDr %in% spr$IDr]

# Filter columns so that I only use sprat and herring IDr's that are in the condition data (don't need the others!)
spr_sum <- spr_sum %>% filter(IDr %in% pred_grid$IDr)
her_sum <- her_sum %>% filter(IDr %in% pred_grid$IDr)

# Select columns from pelagic data to go in dat
spr_sub <- spr_sum %>% dplyr::select(IDr, biomass_spr)
her_sub <- her_sum %>% dplyr::select(IDr, biomass_her)

# Now join dat and sprat data
pred_grid <- left_join(pred_grid, spr_sub)

# And herring..
pred_grid <- left_join(pred_grid, her_sub)

# Now deal with the NA's
unique(is.na(spr_sum$biomass_spr))
unique(is.na(her_sum$biomass_her))

unique(is.na(pred_grid$biomass_spr))
unique(is.na(pred_grid$biomass_her))

# The NA's I have in the DAT are missing pelagic data, i.e. not 0's!
# I will keep them as NA here, because else the prediction grid won't be complete in space
# For visualizing predictions, I will replace NA with the mean in that sub_divivsion, but
# for the data I will drop NA. Hence I do not do it here!

# Now add in the sub division values
biomass_spr_sd <- read_xlsx("data/BIAS/N and B per SD 1991-2020.xlsx",
                            sheet = 4) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("Year" = "ANNUS") %>% 
  mutate_at(vars(`AGE1`, `AGE2`, `AGE3`, `AGE4`, `AGE5`, `AGE6`, `AGE7`, `AGE8+`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(sub_div = as.factor(sub_div),
         Species = "Sprat",
         biomass_spr_sd = `AGE1`+`AGE2`+`AGE3`+`AGE4`+`AGE5`+`AGE6`+`AGE7`+`AGE8+`, # omitting `0+` here
         ID_sd_year = paste(sub_div, Year, sep = ".")) %>% # Make new ID
  dplyr::select(biomass_spr_sd, ID_sd_year)

biomass_her_sd <- read_xlsx("data/BIAS/N and B per SD 1991-2020.xlsx",
                            sheet = 3) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("Year" = "ANNUS") %>% 
  mutate_at(vars(`AGE1`, `AGE2`, `AGE3`, `AGE4`, `AGE5`, `AGE6`, `AGE7`, `AGE8+`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(sub_div = as.factor(sub_div),
         Species = "Sprat",
         biomass_her_sd = `AGE1`+`AGE2`+`AGE3`+`AGE4`+`AGE5`+`AGE6`+`AGE7`+`AGE8+`, # omitting `0+` here
         ID_sd_year = paste(sub_div, Year, sep = ".")) %>% # Make new ID
  dplyr::select(biomass_her_sd, ID_sd_year)

# Add in the same id to the pred_grid
pred_grid <- pred_grid %>% mutate(ID_sd_year = paste(sub_div, year, sep = "."))

pred_grid <- left_join(pred_grid, biomass_spr_sd)

pred_grid <- left_join(pred_grid, biomass_her_sd)

# Plot
pred_grid %>%
  ggplot(., aes(X, Y, fill = biomass_spr_sd)) +
  geom_raster() +
  scale_fill_viridis() +
  facet_wrap(~ year, ncol = 5) +
  labs(x = "lon", y = "lat") +
  theme(axis.text.x = element_text(angle = 90)) +
  NULL

pred_grid %>%
  ggplot(., aes(X, Y, fill = biomass_her_sd)) +
  geom_raster() +
  scale_fill_viridis() +
  facet_wrap(~ year, ncol = 5) +
  labs(x = "lon", y = "lat") +
  theme(axis.text.x = element_text(angle = 90)) +
  NULL
```

## Cod and flounder biomass densities

```{r cod and flounder densities}
# This is so that we can standardize the prediction grid with respect to the data
density <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/mdat_cpue_q_1_4.csv")

density <- density %>% filter(quarter == 4)

# Load models
mcod <- readRDS("output/mcod.rds")
mfle <- readRDS("output/mfle.rds")

# We need to scale the grid with respect to the mean and sd in the data to it was fitted to
density_mean_depth <- mean(density$depth)
density_sd_depth <- sd(density$depth)

# Scale depth in the pred grid
pred_grid <- pred_grid %>%
  mutate(depth_sc = (depth - density_mean_depth) / density_sd_depth)

hist((density$depth - mean(density$depth)) / sd(density$depth))
hist(pred_grid$depth_sc)

# Predict from the density models
cpue_cod <- exp(predict(mcod, newdata = pred_grid)$est)
cpue_fle <- exp(predict(mfle, newdata = pred_grid)$est)

pred_grid$density_cod <- cpue_cod
pred_grid$density_fle <- cpue_fle

# Inspect
ggplot(pred_grid, aes(log(density_cod))) + geom_histogram()
ggplot(pred_grid, aes(log(density_fle))) + geom_histogram()
```

## Large scale variables

```{r large scale variables}
pred_grid <- pred_grid %>% 
  drop_na(depth, temp, oxy, density_saduria, density_cod, density_fle) %>% 
  group_by(year, ices_rect) %>% 
  mutate(depth_rec = median(depth),
         temp_rec = median(temp),
         oxy_rec = median(oxy),
         density_cod_rec = median(density_cod),
         density_fle_rec = median(density_fle),
         density_saduria_rec = median(density_saduria)) %>% 
  ungroup() %>% 
  group_by(year, sub_div) %>% 
  mutate(depth_sd = median(depth),
         temp_sd = median(temp),
         oxy_sd = median(oxy),
         density_cod_sd = median(density_cod),
         density_fle_sd = median(density_fle),
         density_saduria_sd = median(density_saduria))
```

## Save

```{r save}
# Remove variables and save

pred_grid <- pred_grid %>% dplyr::select(-deep, -subdiv, -subdiv2, -IDr, -ID_sd_year,
                                         -depth_sc)

pred_grid_93_06 <- pred_grid %>% filter(year < 2007)
pred_grid_07_19 <- pred_grid %>% filter(year > 2006)

write.csv(pred_grid_93_06, file = "data/for_analysis/pred_grid_(1_2).csv", row.names = FALSE)
write.csv(pred_grid_07_19, file = "data/for_analysis/pred_grid_(2_2).csv", row.names = FALSE)
```

## Make some plots

```{r extra plots}
 # Oxygen vs depth
ggplot(pred_grid, aes(depth, oxy)) + geom_point()

# Oxygen vs year
pred_grid %>% 
  drop_na(oxy) %>% 
  group_by(year) %>% 
  summarise(mean_oxy = mean(oxy),
            sd_oxy = sd(oxy)) %>% 
  ggplot(., aes(year, mean_oxy)) +
  geom_point() + 
  geom_errorbar(aes(x = year, ymin = mean_oxy - sd_oxy, ymax = mean_oxy + sd_oxy, width = 0), alpha = 0.5) + 
  stat_smooth(method = "gam", formula = y ~ s(x, k = 3), color = "tomato") +
  labs(y = "Mean 02 [ml/L]", x = "Year") +  
  labs(y = expression(paste("Mean O" [2], " [ml/L]", sep = "")),
       x = "Year") + 
  theme(legend.position = c(0.8, 0.5))

ggsave("figures/supp/env_oxy.png", width = 6.5, height = 6.5, dpi = 600)

# Oxygen vs year and sd
theme_plot <- function(base_size = 10, base_family = "") {
  theme_light(base_size = 10, base_family = "") +
    theme(
      axis.text.x = element_text(angle = 90),
      axis.text = element_text(size = 8),
      legend.text = element_text(size = 8),
      legend.title = element_text(size = 8),
      legend.position = "bottom",
      legend.key.height = unit(0.2, "cm"),
      legend.margin = margin(0, 0, 0, 0),
      legend.box.margin = margin(-5, -5, -5, -5),
      strip.text = element_text(size = 8, colour = 'gray10', margin = margin()),
      strip.background = element_rect(fill = "grey95")
      )
}

pred_grid %>% 
  drop_na(oxy, sub_div) %>% 
  filter(!sub_div == 22) %>% 
  group_by(year, sub_div) %>% 
  summarise(mean_oxy = mean(oxy),
            sd_oxy = sd(oxy)) %>% 
  ggplot(., aes(year, mean_oxy, color = sub_div, fill = sub_div)) +
  geom_point() + 
  facet_wrap(~ sub_div) +
  # geom_errorbar(aes(x = year, ymin = mean_oxy - sd_oxy, ymax = mean_oxy + sd_oxy, width = 0),
  #               alpha = 0.5) + 
  stat_smooth(method = "gam", formula = y ~ s(x, k = 3)) +
  labs(y = "Mean 02 [ml/L]", x = "Year") +  
  labs(y = expression(paste("Mean O" [2], " [ml/L]", sep = "")), x = "Year") +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  theme_plot() +
  theme(aspect.ratio = 1,
        axis.text.x = element_text(angle = 0),
        legend.position = "bottom",
        legend.key.size = unit(0.6, "cm"),
        legend.text = element_text(size = 8),
        plot.margin = unit(c(0, 0, 0, 0), "cm")) +
  NULL

ggsave("figures/supp/env_oxy_sd.png", width = 6.5, height = 6.5, dpi = 600)

# Oxygen vs year and sd by depth
pred_grid %>% 
  drop_na(oxy, sub_div) %>% 
  mutate(deep = ifelse(depth < 50, "N", "Y")) %>% 
  filter(!sub_div == 22) %>% 
  group_by(year, sub_div, deep) %>% 
  summarise(mean_oxy = mean(oxy),
            sd_oxy = sd(oxy)) %>% 
  ggplot(., aes(year, mean_oxy, color = deep)) +
  geom_point() + 
  facet_wrap(~ sub_div) +
  # geom_errorbar(aes(x = year, ymin = mean_oxy - sd_oxy, ymax = mean_oxy + sd_oxy, width = 0),
  #               alpha = 0.5) + 
  stat_smooth(method = "gam", formula = y ~ s(x, k = 3)) +
  labs(y = "Mean 02 [ml/L]", x = "Year") +  
  labs(y = expression(paste("Mean O" [2], " [ml/L]", sep = "")), x = "Year") +
  theme(strip.text = element_text(colour = 'black'),
        strip.background = element_rect(fill = "grey90"))

ggsave("figures/supp/env_oxy_sd_depth.png", width = 6.5, height = 6.5, dpi = 600)
```


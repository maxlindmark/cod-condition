---
title: "Cod cpue in relation to oxygen"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  df_print: paged
pdf_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.asp = 0.618,
  fig.align ='center'
)
```

# Read cpue data and fit model
```{r lib, message=FALSE}
# Load libraries, install if needed
library(tidyverse); theme_set(theme_classic())
library(readxl)
library(tidylog)
library(RCurl)
library(viridis)
library(RColorBrewer)
library(patchwork)
library(janitor)
library(icesDatras)
library(mapdata)
library(patchwork)
library(rgdal)
library(raster)
library(sf)
library(rgeos)
library(chron)
library(lattice)
library(ncdf4)
library(sdmTMB) # remotes::install_github("pbs-assess/sdmTMB")
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(mapplots)
library(qwraps2) # To load entire cache in interactive r session, do: qwraps2::lazyload_cache_dir(path = "R/analysis/cpue_cod_cache/html")
```


### MOVE THIS TO A NEW DATA CLEANING SCRIPT

```{r data}
# For adding maps to plots
world <- ne_countries(scale = "medium", returnclass = "sf")

# Specify map ranges
# ymin = 54; ymax = 58; xmin = 9.5; xmax = 22
ymin = 54; ymax = 58; xmin = 12.5; xmax = 22

# Make plot function
plot_map_raster <- function(dat, column = "est") {
  ggplot(dat, aes_string("X", "Y", fill = column)) +
    geom_raster() +
    facet_wrap(~year) +
    coord_fixed() +
    scale_fill_viridis_c() +
    geom_sf(data = world, inherit.aes = F, size = 0.2) +
    coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax))
}

# Read data
cov_dat <- read.csv("data/DATRAS_cpue_length_haul/CPUE per length per haul per hour_2020-09-25 16_15_36.csv")

# Remove hauls from outside the study area and select only quarter 4
cov_dat <- cov_dat %>% 
  filter(ShootLat < 58) %>% 
  mutate(kattegatt = ifelse(ShootLat > 56 & ShootLong < 14, "Y", "N")) %>% 
  filter(kattegatt == "N") %>% 
  filter(Quarter == 4) %>% 
  dplyr::select(-kattegatt)

# I am now going to assume that a haul that is present in the condition data but not
# in this covariate data means that the catch is 0
cov_dat %>% arrange(CPUE_number_per_hour)
cov_dat %>% filter(CPUE_number_per_hour == 0)

# Create a new ID column. Note that I can't define a single ID column that works for
# all data sets. The ID that I used for the Exchange data cannot be applied here. I
# need to come up with a new ID here. Run this to see common columns:
# colnames(dat)[colnames(dat) %in% colnames(cov_dat)]
# First filter by species and convert length to cm, then add in ID

cod <- cov_dat %>%
  filter(Species == "Gadus morhua") %>% 
  mutate(length_cm = LngtClass/10) %>% 
  mutate(ID2 = paste(Year, Quarter, Ship, Gear, HaulNo, Depth, ShootLat, ShootLong, sep = "."))

# First check if this is unique by haul. Then I should get 1 row per ID and size...
cod %>%
  group_by(ID2, LngtClass) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  distinct(n, .keep_all = TRUE) %>% 
  as.data.frame()

# Now calculate the mean CPUE per haul
cod_full <- cod %>% 
  arrange(ID2) %>% 
  group_by(ID2) %>% 
  mutate(CPUE_number_per_hour_tot = sum(CPUE_number_per_hour)) %>% 
  ungroup() %>% 
  distinct(ID2, .keep_all = TRUE)

# Test it worked, first by plotting n rows per ID
cod_full %>% group_by(ID2) %>% mutate(n = n()) %>% 
  ggplot(., aes(factor(n))) + geom_bar()

# Next by calculating an example
id <- unique(cod_full$ID2)[99]

cod_full %>%
  filter(ID2 == id) %>%
  dplyr::select(ID2, CPUE_number_per_hour_tot)

sum(filter(cod, ID2 == id)$CPUE_number_per_hour)

# Correct! The data subset yields the same 
cod <- cod_full %>%
  rename("cpue" = "CPUE_number_per_hour_tot",
         "year" = "Year",
         "lat" = "ShootLat",
         "lon" = "ShootLong",
         "quarter" = "Quarter",
         "depth" = "Depth") %>% 
  dplyr::select(cpue, year, lat, lon, quarter, depth)
```

Read the prediction grids:

```{r read and process prediction grid, message=FALSE, warning=FALSE}
# And now read in pred_grid2 which has oxygen values at location and time and depth:
pred_grid2 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/pred_grid2.csv")

pred_grid2 <- pred_grid2 %>%
  mutate(ln_length_cm = log(1)) %>% # For now we'll predict changes in the intercept ("condition factor")
  mutate(X = lon, Y = lat, year = as.integer(year)) %>% 
  filter(year %in% c(unique(cod$year))) %>% 
  mutate(depth_st = (depth - mean(cod$depth))/sd(cod$depth),
         oxy_st = (oxy - mean(cod$oxy))/sd(cod$oxy)) # Need to scale these to the mean and sd in the data!
```

Standardize data with respect to prediction grid:

```{r prep data for cod models, cache=TRUE}
# Standardize variables with respect to the prediction grid
pred_grid2 <- pred_grid2 %>% drop_na(depth)

cod <- cod %>%
  drop_na(depth) %>% 
  mutate(depth_st = (depth - mean(pred_grid2$depth))/sd(pred_grid2$depth),
         depth_st_sq = depth_st*depth_st)
```

Prepare oxygen data. First get a dataframe with all oxygen values in the across the grid, but also match CPUE data with the local oxygen value (largely following the process in the data collation script)

```{r prepare oxygen data, cache=TRUE, message=FALSE}
# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1603971995426.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

# We only use Quarter 4 in this analysis, so now we want to loop through each time step,
# and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep <- which(months > 9)

oxy_q4 <- oxy_array[, , index_keep]

months_keep <- months[index_keep]

years_keep <- years[index_keep]

# Now we have an array with only Q4 data...
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq <- seq(1, dim(oxy_q4)[3], by = 3)

# Create objects that will hold data
dlist <- list()
oxy_10 <- c()
oxy_11 <- c()
oxy_12 <- c()
oxy_ave <- c()

# Loop through the vector sequence with every third value, then take the average of
# three consecutive months (i.e. q4)
for(i in loop_seq) {

  oxy_10 <- oxy_q4[, , (i)]
  oxy_11 <- oxy_q4[, , (i + 1)]
  oxy_12 <- oxy_q4[, , (i + 2)]

  oxy_ave <- (oxy_10 + oxy_11 + oxy_12) / 3

  list_pos <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)

  dlist[[list_pos]] <- oxy_ave

}

# Now name the lists with the year:
names(dlist) <- unique(years_keep)

# Now I need to make a loop where I extract the raster value for each year...
# The condition data is called dat so far in this script

# Filter years in the condition data frame to only have the years I have oxygen for
d_sub_oxy <- cod %>% filter(year %in% names(dlist)) %>% droplevels()

# Create data holding object
data_list <- list()

# ... And for the oxygen raster
raster_list <- list()

# Create factor year for indexing the list in the loop
d_sub_oxy$Year_f <- as.factor(d_sub_oxy$year)

# Loop through each year and extract raster values for the condition data points
for(i in unique(d_sub_oxy$Year_f)) {

  # Subset a year
  oxy_slice <- dlist[[i]]

  # Create raster for that year (i)
  r <- raster(t(oxy_slice), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
              crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))

  # Flip...
  r <- flip(r, direction = 'y')

  plot(r, main = i)

  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice <- d_sub_oxy %>% filter(Year_f == i) %>% dplyr::select(lon, lat)

  # Make into a SpatialPoints object
  data_sp <- SpatialPoints(d_slice)

  # Extract raster value (oxygen)
  rasValue <- raster::extract(r, data_sp)

  # Now we want to plot the results of the raster extractions by plotting the condition
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df <- as.data.frame(data_sp)

  # Add in the raster value in the df holding the coordinates for the condition data
  d_slice$oxy <- rasValue

  # Add in which year
  d_slice$year <- i

  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index <- as.numeric(as.character(d_slice$year[1])) - 1992

  # Add each years' data in the list
  data_list[[index]] <- d_slice

  # Save to check each year is ok! First convert the raster to points for plotting
  # (so that we can use ggplot)
  map.p <- rasterToPoints(r)

  # Make the points a dataframe for ggplot
  df_rast <- data.frame(map.p)

  # Rename y-variable and add year
  df_rast <- df_rast %>% rename("oxy" = "layer") %>% mutate(year = i)

  # Add each years' raster data frame in the list
  raster_list[[index]] <- df_rast

  # Make appropriate column headings
  colnames(df_rast) <- c("Longitude", "Latitude", "oxy")

  # Now make the map
  ggplot(data = df_rast, aes(y = Latitude, x = Longitude)) +
    geom_raster(aes(fill = oxy)) +
    geom_point(data = d_slice, aes(x = lon, y = lat, fill = oxy),
               color = "black", size = 5, shape = 21) +
    theme_bw() +
    geom_sf(data = world, inherit.aes = F, size = 0.2) +
    coord_sf(xlim = c(min(cod$lon), max(cod$lon)),
             ylim = c(min(cod$lat), max(cod$lat))) +
    scale_colour_gradientn(colours = rev(terrain.colors(10)),
                           limits = c(-200, 400)) +
    scale_fill_gradientn(colours = rev(terrain.colors(10)),
                         limits = c(-200, 400)) +
    NULL

    ggsave(paste("figures/supp/cpue_oxygen_rasters/", i,".png", sep = ""),
         width = 6.5, height = 6.5, dpi = 600)

}

# Now create a data frame from the list of all annual values
big_dat_oxy <- dplyr::bind_rows(data_list)
big_raster_dat_oxy <- dplyr::bind_rows(raster_list)

# Plot data, looks like there's big inter-annual variation but a negative
# Plot mean oxygen by year - filter to roughly correspond to the survey area:
big_raster_dat_oxy %>%
  mutate(kattegatt = ifelse(y > 56 & x < 14, "Y", "N")) %>% 
  filter(kattegatt == "N") %>% 
  filter(y > 54 & y < 58 & x > 9.5 & x < 22) %>% 
  group_by(year) %>%
  drop_na(oxy) %>%
  summarise(mean_oxy = mean(oxy),
            sd_oxy = sd(oxy)) %>%
  mutate(year_num = as.numeric(year)) %>%
  ggplot(., aes(year_num, mean_oxy)) +
  #geom_errorbar(aes(ymin = mean_oxy - sd_oxy, ymax = mean_oxy + sd_oxy)) + 
  geom_point(size = 2) +
  stat_smooth(method = "lm") +
  NULL

# Plot number of cells with less than 0 oxygen
big_raster_dat_oxy %>%
  mutate(kattegatt = ifelse(y > 56 & x < 14, "Y", "N")) %>% 
  filter(kattegatt == "N") %>% 
  filter(y > 54 & y < 58 & x > 9.5 & x < 22) %>% 
  group_by(year) %>%
  drop_na(oxy) %>%
  mutate(dead = ifelse(oxy < 0, "Y", "N")) %>%
  filter(dead == "Y") %>%
  mutate(n = n(),
         year_num = as.numeric(year)) %>%
  ggplot(., aes(year_num, n)) +
  geom_point(size = 2) +
  stat_smooth(method = "lm") +
  NULL

# Now add in the new oxygen column in the original data:
str(d_sub_oxy)
str(big_dat_oxy)

# Create an ID for matching the oxygen data with the condition data
cod$id_oxy <- paste(cod$year, cod$lon, cod$lat, sep = "_")
big_dat_oxy$id_oxy <- paste(big_dat_oxy$year, big_dat_oxy$lon, big_dat_oxy$lat, sep = "_")

# Which id's are not in the condition data (dat)?
ids <- dat$id_oxy[!dat$id_oxy %in% c(big_dat_oxy$id_oxy)]

unique(ids)

# Select only the columns we want to merge
big_dat_sub_oxy <- big_dat_oxy %>% dplyr::select(id_oxy, oxy)

# Remove duplicate ID (one oxy value per id)
big_dat_sub_oxy2 <- big_dat_sub_oxy %>% distinct(id_oxy, .keep_all = TRUE)

# Join the data with raster-derived oxygen with the full condition data
cod <- left_join(cod, big_dat_sub_oxy2, by = "id_oxy")

cod <- cod %>%
  drop_na(oxy) %>% 
  mutate(oxy_st = (oxy - mean(oxy))/sd(oxy),
         oxy_st_sq = oxy_st*oxy_st)
```

Make barrier spde mesh:

```{r make barrier spde mesh, results='hide', cache=TRUE, message=FALSE}
# Crop the polygon for plotting and efficiency:
baltic_coast <- suppressWarnings(suppressMessages(
  st_crop(world,
          c(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax))))

crs <- 4326 # https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset#Common_EPSG_codes, WGS84

st_crs(baltic_coast) <- 4326 # 'WGS84'; necessary on some installs
baltic_coast <- st_transform(baltic_coast, crs)

# Project our survey data coordinates:
survey <- cod %>% dplyr::select(lon, lat, cpue) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) 

# Prepare for making the mesh
# First, we will extract the coordinates:
surv_coords <- st_coordinates(survey)

spde <- make_mesh(cod, xy_cols = c("lon", "lat"),
                  n_knots = 200, 
                  type = "kmeans", seed = 42)

# Add on the barrier mesh component:
bspde <- add_barrier_mesh(
  spde, baltic_coast, range_fraction = 0.2,
  proj_scaling = 1, plot = TRUE
)

# In the above, the grey dots are the centre of triangles that are in the
# ocean. The red crosses are centres of triangles that are over land. The
# spatial range will be assumed to be 0.2 (`range_fraction`) over land compared
# to over water.

# We can make a more advanced plot if we want:
# mesh_df_water <- bspde$mesh_sf[bspde$normal_triangles, ]
# mesh_df_land <- bspde$mesh_sf[bspde$barrier_triangles, ]

# Now, when we fit our model with the new mesh, it will automatically
# include a barrier structure in the spatial correlation:
```

Fit the models of cpue

######### RTEMOVE INTERCEPT FORM THESE MODELS1!!!!!!!!

```{r fit, results='hide', cache=TRUE, message=FALSE}
# Depth squared
m1 <- sdmTMB(cpue ~ 0 + as.factor(year) + depth_st + depth_st_sq, data = cod,
             spde = bspde, family = tweedie(link = "log"),
             ar1_fields = TRUE, include_spatial = TRUE, time = "year",
             spatial_only = FALSE, newton_steps = 1, reml = FALSE)

# Depth spline
m2 <- sdmTMB(cpue ~ 0 + as.factor(year) + s(depth_st, k = 3), data = cod,
             spde = bspde, family = tweedie(link = "log"),
             ar1_fields = TRUE, include_spatial = TRUE, time = "year",
             spatial_only = FALSE, newton_steps = 1, reml = FALSE)

# Depth squared + oxy squared
m3 <- sdmTMB(cpue ~ 0 + as.factor(year) + depth_st + depth_st_sq + oxy_st + oxy_st_sq, data = cod,
             spde = bspde, family = tweedie(link = "log"),
             ar1_fields = TRUE, include_spatial = TRUE, time = "year",
             spatial_only = FALSE, newton_steps = 1, reml = FALSE)

# Depth spline + oxy spline
m4 <- sdmTMB(cpue ~ 0 + as.factor(year) + s(depth_st, k = 3) + s(oxy_st, k = 3), data = cod,
             spde = bspde, family = tweedie(link = "log"),
             ar1_fields = TRUE, include_spatial = TRUE, time = "year",
             spatial_only = FALSE, newton_steps = 1, reml = FALSE)

#tidy(mcod, conf.int = TRUE)
cod$residualsm1 <- residuals(m1)
cod$residualsm2 <- residuals(m2)
cod$residualsm3 <- residuals(m3)
cod$residualsm4 <- residuals(m4)
qqnorm(cod$residualsm1); abline(a = 0, b = 1)
qqnorm(cod$residualsm2); abline(a = 0, b = 1)
qqnorm(cod$residualsm3); abline(a = 0, b = 1)
qqnorm(cod$residualsm4); abline(a = 0, b = 1)

AIC(m1, m2, m3, m4) %>% arrange(AIC)
```

Check the AR1 parameter (`rho` is `ar_phi` on the -1 to 1 scale):

```{r check AR1 estimate}
tidy(m4, effects = "ran_pars", conf.int = TRUE) %>% filter(term == "rho")
```

Predict and extract CPUE-weighted mean oxygen and depth per prediction grid

```{r predict}
pred_grid2 <- pred_grid2 %>%
  mutate(X = lon, Y = lat, year = as.integer(year)) %>% 
  filter(year %in% c(unique(cod$year))) %>% 
  mutate(depth_st = (depth - mean(cod$depth))/sd(cod$depth),
         oxy_st = (oxy - mean(cod$oxy))/sd(cod$oxy),
         depth_st_sq = depth_st*depth_st) %>% # Need to scale these to the mean and sd in the data!
  drop_na(oxy_st)

predcod <- predict(m4, newdata = pred_grid2)
```

Plot on map

```{r plot}
predcod <- predcod %>% mutate(est2 = ifelse(depth > 120, NA, est))

# predcod %>% mutate(est2_resp = exp(est2)) %>% arrange(exp(est2_resp)) %>% dplyr::select(est2_resp) %>% head(20)
# predcod %>% mutate(est2_resp = exp(est2)) %>% arrange(desc(est2_resp)) %>% dplyr::select(est2_resp) %>% head(20)

# Oxygen
predcod %>% 
  #filter(year %in% c("1993", "2018")) %>% 
  filter(exp(est2) < 2000) %>% 
  filter(exp(est2) > 0.01) %>% # remove some extremes to better see trends in space and time
  ggplot(., aes(X, Y, fill = exp(est2))) +
  geom_raster() +
  facet_wrap(~year, ncol = 5) +
  scale_fill_viridis(option = "magma", 
                     name = "cpue") + 
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Prediction (random + fixed)") +
  labs(x = "lon", y = "lat")
```

We can also plot the annual index

```{r annual condition factor, message=FALSE, cache=TRUE}
# From these models, predict annual condition factor
# Grabbing the number of cells to help with calculating the average
ncells <- filter(pred_grid2, year == max(pred_grid2$year)) %>% nrow()

# Use the `area` argument here to turn the total into an average by giving it one over the number of cells
pred_grid2 <- pred_grid2 %>% filter(depth < 120)

preds_mcod <- predict(mcod, newdata = pred_grid2, return_tmb_object = TRUE, area = 1/ncells)

# Make a little helper function... bias correction shouldn't do anything here because of the identity link
get_average_index <- function(obj, level = 0.95, ...)  {
  sdmTMB:::get_generic(obj, value_name = "link_total",
    bias_correct = FALSE, level = level, trans = I, ...)
}

avg_mcod <- get_average_index(preds_mcod)

avg_mcod <- avg_mcod %>% mutate(est_response = exp(est),
                                lwr_response = exp(lwr),
                                upr_response = exp(upr))

avg_mcod %>%
  ggplot(., aes(year, est_response)) +
  ylab("cpue") +
  geom_line() + 
  geom_ribbon(aes(ymin = lwr_response, ymax = upr_response), alpha = 0.4) +
  theme(axis.text.x = element_text(angle = 30),
        legend.position = c(0.8, 0.8)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(x = "Year") +
  NULL
```

Calculate cpue-weighted mean oxygen-at-location

```{r cpue-weighted mean oxygen plot}
predcod %>% 
  drop_na(est2) %>% 
  group_by(year) %>% 
  #filter(exp(est2) < 1000) %>% # remove some extremes to better see trends in space and time 
  #filter(exp(est2) > 0.05) %>% # remove some extremes to better see trends in space and time
  summarise(oxygen_wm = weighted.mean(oxy, exp(est2))) %>% 
  ggplot(., aes(year, oxygen_wm)) +
  geom_point(size = 3) +
  stat_smooth(method = "lm") +
  theme(axis.text.x = element_text(angle = 30),
        legend.position = c(0.8, 0.8)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("CPUE weighted average oxygen at location") 
```

Calculate cpue-weighted mean depth-at-location
```{r cpue-weighted mean depth plot}
predcod %>% 
  drop_na(est2) %>% 
  group_by(year) %>% 
  #filter(exp(est2) < 1000) %>% # remove some extremes to better see trends in space and time 
  #filter(exp(est2) > 0.05) %>% # remove some extremes to better see trends in space and time
  summarise(depth_wm = weighted.mean(depth, exp(est2))) %>% 
  ggplot(., aes(year, depth_wm)) +
  geom_point(size = 3) +
  stat_smooth(method = "lm") +
  theme(axis.text.x = element_text(angle = 30),
        legend.position = c(0.8, 0.8)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("CPUE weighted average depth at location") 
```

Marginal effect of depth

```{r marginal effects depth, message=FALSE, cache=TRUE}
# Prepare prediction data frame
nd_dep <- data.frame(depth = seq(min(cod$depth), max(cod$depth), length.out = 100))
nd_dep <- nd %>%
  mutate(year = 2003L,
         depth_st = (depth - mean(depth))/sd(depth),
         depth_st_sq = depth_st*depth_st,
         oxy_st = 0,
         oxy_st_sq = 0)

# Predict 
p_margin_dep <- predict(m4, newdata = nd_dep, se_fit = TRUE, re_form = NA)

ggplot(p_margin_dep, aes(depth, exp(est),
  ymin = exp(est) - 1.96 * exp(est_se), ymax = exp(est) + 1.96 * exp(est_se))) +
  geom_line() + 
  geom_ribbon(alpha = 0.4)
```

Marginal effect of oxygen

```{r marginal effects oxygen, message=FALSE, cache=TRUE}
# Prepare prediction data frame
nd_oxy <- data.frame(oxy = seq(min(cod$oxy), max(cod$oxy), length.out = 100))
nd_oxy <- nd_oxy %>%
  mutate(year = 2003L,
         depth_st = 0,
         depth_st_sq = 0,
         oxy_st = (oxy - mean(oxy))/sd(oxy),
         oxy_st_sq = oxy_st*oxy_st)

# Predict 
p_margin_oxy <- predict(m4, newdata = nd_oxy, se_fit = TRUE, re_form = NA)

ggplot(p_margin_oxy, aes(oxy, exp(est),
  ymin = exp(est) - 1.96 * exp(est_se), ymax = exp(est) + 1.96 * exp(est_se))) +
  geom_line() + 
  geom_ribbon(alpha = 0.4)
```


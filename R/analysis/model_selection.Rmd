---
title: "Model selection"
author: "Max Lindmark & Sean Andersson"
date: "9/18/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.asp = 0.618,
  fig.align ='center'
)
```

## Background
The body condition and growth of Eastern Baltic cod (*Gadus morhua*) has declined steadily since the regime shift in the early 1990's to a degree that the stock now can be viewed as collapsed. Several hypotheses have been put forward, including changes in overlap with pelagic prey or lack of appropriately sized prey (e.g. Casini *et al*, 2016; Gårdmark *et al*, 2015), reduced oxygen levels decreasing habitat quality and leading to contraction of the distributional range thus increasing competition (e.g. Casini *et al*, 2016), increased competition for benthic food sources with flounder (Orio *et al*, 2019; Orio 2020) as well as increased intraspecific competition and growth bottlenecks within the population (Svedäng & Hornborg, 2014).

However, these potential explanatory variables have not been evaluated on a fine spatial scale, even though factors such as competition, food availability and local habitat quality likely occur on fine spatial scales. Instead, averages over larger spatial areas (e.g. ICES subdivisions) have been the variables in previous comparisons. Moreover, the ability of each of proposed explanatory variable (linked to a hypothesis) to explain variation in the condition of cod has not been compared in a standardized way, and has not been contrasted to residual spatial and spatiotemporal variation.

### Aim
In this study I have compiled data for individual-level condition in the whole Baltic Sea (essentially everything south and east of [Kattegatt](http://stateofthebalticsea.helcom.fi/in-brief/our-baltic-sea/)) and matched that with predictor variables (on a haul level) representing different ecological hypotheses regarding drivers of variation in cod condition.

To account for spatial and temporal autocorrelation using data at this scale, we apply spatiotemporal predictive-process GLMMs using the R-package [sdmTMB](https://github.com/pbs-assess/sdmTMB). This modeling framework allows evaluation of how much of the variation in condition can be explained by covariates, spatial (unmeasured variation in condition that is stable over time) and spatiotemporal variation (unmeasured variation in condition that changes between years).

In this script, we compare models with different sets of covariates and evaluate if they contribute to a parsimonious fit.

## Methods
### Modeling framework
In fishes, weight is typically assumed to vary log-normally around an average allometric function of length: $w=al^b$, where $w$ is weight in grams, $l$ is length in cm, $b$ is the allometric length exponent and $a$ is the condition factor in unit $g/L^b$ (Froese *et al*., 2014; Grüss *et al*., 2020). Typically this relationship is linearized by taking logs on both sides: $\operatorname{log}(w)=a+b\operatorname{log}(l)$. Le Cren's condition index is defined as the residuals from this length-weight relationship.

We model this individual-level relationship with a spatiotemporal GLMM of the form (minor deviations from this model are discussed in this document):

$$
\operatorname{log}(w_{s,t})\sim\operatorname{Student-t}(\mu_{s,t},\sigma,\nu)\\
\mu_{s,t} = \alpha_t + \beta{depth} + \boldsymbol\omega_s + \boldsymbol\epsilon_{s,t} + \sum^{n_k}_{k=1}\boldsymbol\gamma_k\boldsymbol{X_k} + \beta \operatorname{log}(l),
$$ 

where the degrees of freedom, $\nu$, are set to 2 and $\phi$ is the standard deviation. $\alpha_t$ is a time-varying intercept:

$$
\alpha_t \sim \operatorname{Normal}(\alpha_{t-1}, \sigma^2_\gamma).
$$

$\beta_d$ is the coefficient for $depth$, $\boldsymbol\omega_s$ and $\boldsymbol\epsilon_{s,t}$ represent spatial and spatiotemporal random effects, respectively. $\boldsymbol{X_k}$ is a matrix of $n_k$ measured *additional* covariates and $\boldsymbol\gamma_k$ is the effect of the $k$-th *additional* covariate. $\beta$ is the length-coefficient, corresponding to the allometric exponent $b$. The spatial and spatiotemporal random effects are assumed to be drawn from a multivariate normal distribution:

$$
\boldsymbol\omega \sim \operatorname{MVNormal}(\boldsymbol0, \boldsymbol\Sigma_\omega)\\
\boldsymbol\epsilon_t \sim \operatorname{MVNormal}(\boldsymbol0, \boldsymbol\Sigma_\epsilon).
$$

We also consider the spatiotemporal random effects to be drawn from a multivariate normal distribution following an AR1 process:

$$
\boldsymbol\delta_{t=1} \sim \operatorname{MVNormal}(\boldsymbol0, \boldsymbol\Sigma_\epsilon)\\
\boldsymbol\delta_{t>1} = \phi\boldsymbol\delta_{t-1} + \sqrt{1-\phi^2}\boldsymbol\epsilon_t, \boldsymbol\epsilon_t \sim \operatorname{MVNormal}(\boldsymbol0, \boldsymbol\Sigma_\epsilon).
$$

In the spatial and spatiotemporal random fields, $\Sigma_\omega$ and $\Sigma_\epsilon$ are covariance matricies, where the covariance ($\Phi(s, s')$) between spatial points $s$ and $s'$ is given by a Matérn function:

$$
\Phi(s, s') = \tau^2/\Gamma(\nu)2^{\nu-1}(\kappa d_{jk})^{\nu}K_\nu(\kappa d_{jk}),
$$
where $\tau^2$ is the spatial (marginal) variance. 

This model (first equation) can be viewed as an approximation of Le Cren's condition index (Grüss *et al*., 2020), as the log of the condition factor, i.e. $\operatorname{log}(a)$ or the constant in the allometric relationship, can be defined as: $\operatorname{log}(a) = \alpha_t + \boldsymbol\omega_s + \boldsymbol\epsilon_{s,t} + \sum^{n_k}_{k=1}\boldsymbol\gamma_k\boldsymbol{X_k}$. Thus, Eq. 1 is a model for a spatially and temporally varying condition factor.

## Fit models
### Read data and set up spde mesh
```{r packages, message=FALSE, warning=TRUE}
library(tidyverse); theme_set(theme_classic())
library(tidylog)
library(viridis)
library(sdmTMB) # remotes::install_github("pbs-assess/sdmTMB")
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
library(RColorBrewer)
library(gganimate)
library(gifski)
library(latex2exp)
library(patchwork)
library(png)
library(qwraps2) # To load entire cache in interactive r session, do: qwraps2::lazyload_cache_dir(path = "R/analysis/model_selection_cache/html")

# For adding maps to plots
world <- ne_countries(scale = "medium", returnclass = "sf")

# Specify map ranges
ymin = 54; ymax = 58; xmin = 9.5; xmax = 22
```

Now read data:

```{r read and process data, message=FALSE, warning=FALSE}
d <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/mdat_cond.csv")

# Calculate standardized variables
d <- d %>% 
  mutate(ln_length_cm = log(length_cm),
         ln_weight_g = log(weight_g),
         oxy_st = oxy,
         oxy_rec_st = oxy_rec,
         abun_her_st = abun_her,
         abun_her_sd_st = abun_her_sd,
         abun_spr_st = abun_spr,
         abun_spr_sd_st = abun_spr_sd,
         cpue_cod_st = cpue_cod,
         cpue_cod_rec_st = cpue_cod_rec,
         cpue_fle_st = cpue_fle,
         cpue_fle_rec_st = cpue_fle_rec,
         depth_st = depth) %>%
  mutate_at(c("oxy_st", "oxy_rec_st", "abun_her_st", "abun_her_sd_st", "abun_spr_st",
              "abun_spr_sd_st", "cpue_cod_st", "cpue_cod_rec_st", "cpue_fle_st",
              "cpue_fle_rec_st", "depth_st"),
            ~(scale(.) %>% as.vector)) %>% 
  mutate(year = as.integer(year))
```

Read the prediction grids:

```{r read and process prediction grid, message=FALSE, warning=FALSE}
pred_grid <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/pred_grid.csv")

pred_grid <- pred_grid %>%
  mutate(ln_length_cm = log(1)) %>% # For now we'll predict changes in the intercept ("condition factor")
  mutate(X = lon,
         Y = lat,
         year = as.integer(year),
         depth_st = 0) %>% # In this prediction grid I keep depth at its mean, below I have a more realistic prediction grid
  filter(year %in% c(unique(d$year)))

# And now read in pred_grid2 which has oxygen values at location and time and depth:
pred_grid2 <- readr::read_csv("https://raw.githubusercontent.com/maxlindmark/cod_condition/master/data/for_analysis/pred_grid2.csv")

pred_grid2 <- pred_grid2 %>%
  mutate(ln_length_cm = log(1)) %>% # For now we'll predict changes in the intercept ("condition factor")
  mutate(X = lon, Y = lat, year = as.integer(year)) %>% 
  filter(year %in% c(unique(d$year))) %>% 
  mutate(depth_st = (depth - mean(d$depth))/sd(d$depth),
         oxy_st = (oxy - mean(d$oxy))/sd(d$oxy)) # Need to scale these to the mean and sd in the data!
```

In earlier versions we used this: `spde <- make_mesh(data = d, xy_cols = c("lon", "lat"), n_knots = 110, type = "kmeans", seed = 42)`. But now we have even more islands in the data since I am using also the western Baltic Sea, so we will use the `add_barrier_mesh` to include an island effect (following the example function). Note this also means we can increase the # of knots before hitting convergence issues (how much varies from model to model though!)

```{r make barrier spde mesh, results='hide', message=FALSE}
# Crop the polygon for plotting and efficiency:
baltic_coast <- suppressWarnings(suppressMessages(
  st_crop(world,
          c(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax))))

crs <- 4326 # https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset#Common_EPSG_codes, WGS84

st_crs(baltic_coast) <- 4326 # 'WGS84'; necessary on some installs
baltic_coast <- st_transform(baltic_coast, crs)

# Project our survey data coordinates:
survey <- d %>% dplyr::select(lon, lat, ln_weight_g) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) 

# Prepare for making the mesh
# First, we will extract the coordinates:
surv_coords <- st_coordinates(survey)

spde <- make_mesh(d, xy_cols = c("lon", "lat"),
                  n_knots = 160, # 180 works nice as well but 160 works with the 80% training data
                  type = "kmeans", seed = 42)

# Add on the barrier mesh component:
bspde <- add_barrier_mesh(
  spde, baltic_coast, range_fraction = 0.2,
  proj_scaling = 1, plot = TRUE
)

# In the above, the grey dots are the centre of triangles that are in the
# ocean. The red crosses are centres of triangles that are over land. The
# spatial range will be assumed to be 0.2 (`range_fraction`) over land compared
# to over water.

# We can make a more advanced plot if we want:
mesh_df_water <- bspde$mesh_sf[bspde$normal_triangles, ]
mesh_df_land <- bspde$mesh_sf[bspde$barrier_triangles, ]

# Now, when we fit our model with the new mesh, it will automatically
# include a barrier structure in the spatial correlation:
```

### Default model (no additional covariates)
```{r without covariates, cache=TRUE}
mdef <- sdmTMB(formula = ln_weight_g ~ ln_length_cm + depth_st -1, time_varying = ~ 1, data = d, time = "year",
                spde = bspde, family = student(link = "identity", df = 5), ar1_fields = TRUE,
                include_spatial = TRUE, spatial_trend = FALSE, spatial_only = FALSE,
                silent = TRUE, newton_steps = 1, reml = FALSE)

tidy(mdef, conf.int = TRUE)
```

### Small scale models
#### Full model
```{r full model, cache=TRUE}
mfull <- sdmTMB(formula = ln_weight_g ~ ln_length_cm + depth_st + oxy_st + cpue_fle_st + cpue_cod_st + abun_spr_st + abun_her_st -1, time_varying = ~ 1, data = d, time = "year",
                spde = bspde, family = student(link = "identity", df = 5), ar1_fields = TRUE,
                include_spatial = TRUE, spatial_trend = FALSE, spatial_only = FALSE,
                silent = TRUE, newton_steps = 1, reml = FALSE)

tidy(mfull, conf.int = TRUE)
```

### Large scale models
#### Full model
```{r full model large, cache=TRUE}
mfull_large <- sdmTMB(formula = ln_weight_g ~ ln_length_cm + depth_st + oxy_rec_st + cpue_fle_rec_st + cpue_cod_rec_st + abun_her_sd_st + abun_spr_sd_st -1, time_varying = ~ 1, data = d, time = "year",
                spde = bspde, family = student(link = "identity", df = 5), ar1_fields = TRUE,
                include_spatial = TRUE, spatial_trend = FALSE, spatial_only = FALSE,
                silent = TRUE, newton_steps = 1, reml = FALSE)

tidy(mfull_large, conf.int = TRUE)
```

## Compare models
### AIC
```{r AIC, cache=TRUE}
AIC(mdef, mfull, mfull_large) %>% arrange(AIC)
```
Ok, so it seems that the model with small-scale covariates is preferred. But is the model overfitted, since we do not do any variable selection?

### Evalute out-of-sample predictive accuracy
As we have not landed yet on an approach for variable selection or if we should do ridge regression, I here only fit the full models and explore how well they predict out of sample as a measure of overfitting. So we can perhaps do k-fold cross validation, but that code doesn't exist yet on `sdmTMB`. So for now I simply withold 80% of the data and compare the mean squared error (MSE) for predictions on the training data and predictions on the withheld data. If the MSE is much larger for the withheld data that indicates overfitting (though it is difficult to say how much larger it has to be for it to be overfitting).

```{r full model cv, cache=TRUE}
train_size <- 0.8*nrow(d)

set.seed(42)
train_rows <- sample.int(nrow(d), train_size)
train <- d[train_rows, ]

# Assign rows as either for training or for later prediction (withheld)
d <- d %>% 
  mutate(row_n = 1:n(),
         split = ifelse(row_n %in% train_rows, "train", "withheld"))

# Plot just to see 
d %>% group_by(year, split) %>% summarize(n = n()) %>%
  ggplot(., aes(factor(year), n, fill = factor(split))) +
  geom_bar(stat = "identity", position = "dodge")

# Now I think we need to redo the spde mesh, because we now fit the model using only 80 % of the data
# Project our survey data coordinates:
survey2 <- filter(d, split == "train") %>% dplyr::select(lon, lat, ln_weight_g) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) 

# Prepare for making the mesh
# First, we will extract the coordinates:
surv_coords <- st_coordinates(survey2)

spde2 <- make_mesh(filter(d, split == "train"), xy_cols = c("lon", "lat"),
                   n_knots = 160, 
                   type = "kmeans", seed = 42)

# Add on the barrier mesh component:
bspde2 <- add_barrier_mesh(
  spde2, baltic_coast, range_fraction = 0.2,
  proj_scaling = 1, plot = TRUE
)

# Fit full model (all covariates) using the training part of the data
mfull_train <- sdmTMB(formula = ln_weight_g ~ ln_length_cm + depth_st + oxy_st + cpue_fle_st + cpue_cod_st + abun_spr_st + abun_her_st -1, time_varying = ~ 1, time = "year",
                spde = bspde2, family = student(link = "identity", df = 5), ar1_fields = TRUE,
                include_spatial = TRUE, spatial_trend = FALSE, spatial_only = FALSE,
                silent = TRUE, newton_steps = 1, reml = FALSE,
                data = filter(d, split == "train"))

# Then fit a model using only oxygen as a covariate using the same data
moxy_train <- sdmTMB(formula = ln_weight_g ~ ln_length_cm + depth_st + oxy_st -1, time_varying = ~ 1, time = "year",
                spde = bspde2, family = student(link = "identity", df = 5), ar1_fields = TRUE,
                include_spatial = TRUE, spatial_trend = FALSE, spatial_only = FALSE,
                silent = TRUE, newton_steps = 1, reml = FALSE,
                data = filter(d, split == "train"))

# Now we need to extract the residuals from the predictions on the training data and the withheld data
# First, borrow some functions from sdmTMB to calculate quantile residuals: https://github.com/pbs-assess/sdmTMB/blob/master/R/residuals.R
pt_ls <- function(q, df, mu, sigma) stats::pt((q - mu)/sigma, df)

qres_student <- function(object, y, mu) {
  dispersion <- exp(object$model$par[["ln_phi"]])
  u <- pt_ls(q = y, df = object$tmb_data$df, mu = mu, sigma = dispersion)
  stats::qnorm(u)
}

# Calculate the full models' predictive error
res_full_train <- qres_student(object = mfull_train,
                               y = filter(d, split == "train")$ln_weight_g,
                               mu = predict(mfull_train)$est)

res_full_withheld <- qres_student(object = mfull_train,
                                  y = filter(d, split == "withheld")$ln_weight_g,
                                  mu = predict(mfull_train, newdata = filter(d, split == "withheld"))$est)

mse_full_train <- mean(res_full_train^2)
mse_full_withheld <- mean(res_full_withheld^2)

# Calculate  the oxygen models' predictive error
res_oxy_train <- qres_student(object = moxy_train,
                              y = filter(d, split == "train")$ln_weight_g,
                              mu = predict(moxy_train)$est)

res_oxy_withheld <- qres_student(object = moxy_train,
                                 y = filter(d, split == "withheld")$ln_weight_g,
                                 mu = predict(moxy_train, newdata = filter(d, split == "withheld"))$est)

mse_oxy_train <- mean(res_oxy_train^2)
mse_oxy_withheld <- mean(res_oxy_withheld^2)
```

Compare the relative error on the withheld data to the full data for the full model and the model with oxygen
```{r compare errors between simple and full model}
mse_full_withheld/mse_full_train
mse_oxy_withheld/mse_oxy_train
```

### Plot fixed and random effects
```{r extract coefficients, message=FALSE}
# Extract random and fixed coefficients from the full model
mfull_est <- bind_rows(tidy(mfull, effects = "ran_par", conf.int = TRUE) %>%
                         filter(term %in% c("sigma_O", "sigma_E")),
                       tidy(mfull, effects = "fixed", conf.int = TRUE) %>%
                         filter(!term %in% c("ln_length_cm"))) %>%
  mutate(Model = "Oxygen model") %>%
  mutate(term = factor(term)) %>%
  mutate(term = recode(term,
                     "oxy_st" = "Oxygen",
                     "abun_her_st" = "Herring",
                     "abun_spr_st" = "Sprat",
                     "cpue_cod_st" = "Cod",
                     "cpue_fle_st" = "Flounder",
                     "sigma_O" = 'σ_O',
                     "sigma_E" = 'σ_E',
                     "depth_st" = "Depth"))

# Full model
ggplot(mfull_est, aes(term, estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  geom_hline(yintercept = 0, linetype = 2, color = "gray") +
  ggtitle("Spatial and spatiotemporal variation vs covariates") +
  labs(x = "", y = "Standardized coefficient") +
  theme_classic(base_size = 12) +
  theme(axis.text.x = element_text(angle = 15, size = 10))
```

### Visualize marginal effects of oxygen
```{r marginal effects, message=FALSE, cache=TRUE}
# Prepare prediction data frame
nd_oxy <- data.frame(oxy_st = seq(min(d$oxy_st), max(d$oxy_st), length.out = 100))
nd_oxy$year <- 2003L
nd_oxy$depth_st <- 0
nd_oxy$ln_length_cm <- 0
nd_oxy$cpue_fle_st <- 0
nd_oxy$cpue_cod_st <- 0
nd_oxy$abun_spr_st <- 0
nd_oxy$abun_her_st <- 0

# Predict from full model (AIC-selected)
p_margin_oxy <- predict(mfull, newdata = nd_oxy, se_fit = TRUE, re_form = NA)

ggplot(p_margin_oxy, aes(oxy_st, est,
  ymin = est - 1.96 * est_se, ymax = est + 1.96 * est_se)) +
  geom_ribbon(alpha = 0.4) + geom_line() +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = c(0.1, 0.9))

# What is the difference in predicted weight? Approximately 5%. Still considerable variation!
exp(-4.475 + 2.98*log(50)) / exp(-4.525 + 2.98*log(50))
```

We can also plot the annual condition factor
```{r annual condition factor, message=FALSE,cache=TRUE}
# From these models, predict annual condition factor
# Grabbing the number of cells to help with calculating the average
ncells <- filter(pred_grid2, year == max(pred_grid$year)) %>% nrow()

pred_grid_oxy <- pred_grid2 %>% drop_na(oxy)

pred_grid_oxy$cpue_fle_st <- 0
pred_grid_oxy$cpue_cod_st <- 0
pred_grid_oxy$abun_spr_st <- 0
pred_grid_oxy$abun_her_st <- 0

# For now I set all covariates to zero, but if we want to calculate annual averages with covariates (e.g. oxygen), then we should fit the model with oxygen centered to the mean in the prediction grid.

# Use the `area` argument here to turn the total into an average by giving it one over the number of cells
preds_mfull <- predict(mfull, newdata = pred_grid_oxy, return_tmb_object = TRUE, area = 1/ncells)

# Make a little helper function... bias correction shouldn't do anything here because of the identity link
get_average_condition <- function(obj, level = 0.95, ...)  {
  sdmTMB:::get_generic(obj, value_name = "link_total",
    bias_correct = FALSE, level = level, trans = I, ...)
}

avg_mfull <- get_average_condition(preds_mfull)

avg_mfull %>%
  ggplot(., aes(year, est)) +
  ylab("Average log(condition factor)") +
  geom_point(size = 3) +
  geom_errorbar(aes(x = year, ymax = upr, ymin = lwr),
                width = 0.2, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 30),
        legend.position = c(0.8, 0.8)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(x = "Year") +
  NULL
```

We can make a prediction over a grid with the oxygen values values at each location across time `pred_grid2`, but keeping all other covariates at their means:

```{r predict on grid with oxygen, message=FALSE, fig.width=12}
pred_grid2 <- pred_grid2 %>% 
  mutate(cpue_fle_st = 0,
         cpue_cod_st = 0,
         abun_spr_st = 0,
         abun_her_st = 0)

poxy2 <- predict(mfull, newdata = pred_grid2)

# Replace too-deep predictions with NA
poxy2 <- poxy2 %>% mutate(est2 = ifelse(depth > 120, NA, est))

ggplot(poxy2, aes(X, Y, fill = est2)) +
  geom_raster() +
  facet_wrap(~year, ncol = 5) +
  scale_fill_viridis(option = "magma",
                     name = "log(condition factor)") +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Prediction (random + fixed)") +
  labs(x = "lon", y = "lat")
```

Plot spatial random effects:
```{r spatial omega map, message=FALSE, fig.width=12}
# Replace too-deep predictions with NA and filter a single year
poxy3 <- poxy2 %>% mutate(omega_s2 = ifelse(depth > 120, NA, omega_s)) %>% filter(year == 1999)

ggplot(poxy3, aes(X, Y, fill = omega_s2)) +
  geom_raster() +
  scale_fill_gradient2() +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Spatial random field") +
  theme_classic(base_size = 12) +
  labs(x = "lon", y = "lat")
```

```{r gganimate condition, message=FALSE, include=FALSE}
p <- ggplot(poxy2, aes(X, Y, fill = est2)) +
  geom_raster() +
  scale_fill_viridis(option = "magma",
                     name = "log(condition factor)") +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  labs(x = "lon", y = "lat")

# Here comes the gganimate specific bits
anim <- p +
  labs(title = 'Year: {frame_time}') +
  transition_time(as.integer(year)) +
  ease_aes('linear') +
  theme_classic(base_size = 24)

gganimate::animate(anim, height = 1200, width = 1200)

anim_save(filename = "/Users/maxlindmark/Desktop/R_STUDIO_PROJECTS/cod_condition/output/gif/cond_oxy.gif")
```

Calculate the "spatial trend" from the estimates:

```{r calculate "spatial trend", message=FALSE}
# Fit a linear model to each prediction grid of the estimate over time
# https://community.rstudio.com/t/extract-slopes-by-group-broom-dplyr/2751/7
time_slopes_by_year <- poxy2 %>% 
  drop_na(est2) %>% 
  mutate(id = paste(lon, lat, sep = "_")) %>% 
  split(.$id) %>% 
  purrr::map(~lm(est2 ~ year, data = .x)) %>% 
  purrr::map_df(broom::tidy, .id = 'id') %>%
  filter(term == 'year')
  
# Plot the slopes
time_slopes_by_year %>% 
  separate(id, c("X", "Y"), sep = "_") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  ggplot(., aes(X, Y, fill = estimate)) +
  geom_raster() +
  scale_fill_gradient2(midpoint = 0) +
  #scale_fill_viridis() +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Time slopes by each pred grid") +
  theme_classic(base_size = 12) +
  labs(x = "lon", y = "lat")

# Plot the standard errors
time_slopes_by_year %>% 
  separate(id, c("X", "Y"), sep = "_") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  ggplot(., aes(X, Y, fill = std.error)) +
  geom_raster() +
  scale_fill_viridis() +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("standard error of time slopes by each pred grid") +
  labs(x = "lon", y = "lat")

# Plot the upper CI
time_slopes_by_year %>% 
  separate(id, c("X", "Y"), sep = "_") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  ggplot(., aes(X, Y, fill = estimate + std.error*1.96)) +
  geom_raster() +
  scale_fill_gradient2(midpoint = 0) +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Upper CI of time slopes by each pred grid") +
  labs(x = "lon", y = "lat")

# Lower CI
time_slopes_by_year %>% 
  separate(id, c("X", "Y"), sep = "_") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  ggplot(., aes(X, Y, fill = estimate - std.error*1.96)) +
  geom_raster() +
  scale_fill_gradient2(midpoint = 0) +
  geom_sf(data = world, inherit.aes = F, size = 0.2) +
  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  ggtitle("Lower CI of time slopes by each pred grid") +
  labs(x = "lon", y = "lat")
``` 

